# 1.NLP 기초.
## 2. 단어 토큰화
### 2. 토큰 생성기를 이용한 어휘 구축
#### 2. 두 단어 모음의 중복 측정
- 두 단어 모음 벡터가 어느 정도나 겹치는지 측정한다면 해당 문장들이 단어들을 얼마나 비슷하게 사용하는지 가늠 가능. 그리고 그러한 측도는 두 문장의 의미가 얼마나 비슷한지를 어느 정도 잘 말해 줌. 앞에서 배운 내적을 이용, 원래 예제 문장(sent0)과 몇 가지 새 문장의 단어 모음 벡터 중복도를 측정
- ```python
  >>> df = df.T
  >>> df.sent0.dot(df.sent1)
  >>> df.sent0.dot(df.sent2)
  >>> df.sent0.dot(df.sent3)
  ```
- 위 결과는 두 문장 sent0과 sent2가 하나의 단어를 공통으로 사용하고, sent0과 sent3이 하나의 단어를 공유하는 것을 알림.
- 이러한 단어 중복도는 두 문장의 유사성에 관한 측도이고, 동떨어진 문장 sent1은 제퍼슨 혹은 몬티첼로를 직접 언급하지 않는 유일한 문장임. 이 문장은 다른 문장들과는 완전히 다른 단어들로 어떤 익명의 사람들에 관한 정보를 전달. 
- 다음 예제 코드는 sent0과 sent3이 공유하는 단어, 즉 마지막 내적이 1이 되게 한 단어를 찾는 방법을 보여줌.
- ```python
  >>>[(k,v) for (k,v) in (df.sent0 & df.sent3).items() if v]
  ```
- 이상이 자연어 문서(문장)들에 대한 우리의 첫 번째 벡터 공간 모형(vector space model, VSM)임.
- 이러한 모형의 단어 모음 벡터들에 대해 내적 연산뿐만 아니라 벡터 덧셈, 뺄셈, 논리합(OR), 논리곱(AND)같은 다양한 벡터 연산을 적용 가능. 두 벡터 사이의 유클리드 거리나 각도 같은것도 계산 가능. 문서를 이처럼 이진 벡터로 표현하는 방식의 위력은 대단히 큼. 오랫동안 이것이 문서 조회 및 검색의 주된 수단. 
- 현대적인 CPU들은 예외 없이 이런 이진 벡터들을 효율적으로 해싱하고, 색인화하고, 검색하는데 적합한 메모리 접근 명령들을 갖추고 있음. 원래 그런 CPU 명령들은 다른 목적(RAM에서 자료를 조회하기 위해 메모리 장소를 색인화하는)으로 만들어진 것이지만, 텍스트 조회 및 검색을 위한 이진 벡터 연산들에도 똑같이 효율적.