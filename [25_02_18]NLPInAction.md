# 2. 더 깊은 학습: 신경망 적용
## 6. 단어 벡터를 이용한 추론: word2vec 활용
### 2. 단어 벡터
- 입력으로부터 예측한 출력을 저차원 내부 표현의 입력으로 삼아서 원래의 입력을 다시 예측함으로써 뭔가를 배우는 모형을 자동 부호기(autocoder; 또는 자가부호기)라고 부름. 원래의 입력을 다시 예측한다는 것이 좀 이상할 수는 있지만, 자동부호기가 원래의 질문을 녹음해서 그대로 재생하는 단순한 방식이 아니라 주어진 입력을 더 간결한(낮은 차원의)표현으로 압축, 그것을 최대한 원래의 입력으로 복원한다는 의미
- 다수의 입력에 대해 이런 시도를 반복함으로써 모형 안에는 일종의 압축 알고리즘이 형성. 결과적으로 자동 부호기는 주어진 자료 집합을 간결하게 표현하는 방법을 학습. 문서 같은 고차원 대상의 압축된 표현을 생성하는 비지도 심층 모형에 대한 시작으로 이 자동부호기가 괜찮은 시도. 또한, 자동부호기는 거의 모든 자료 집합에 적용할 수 있기 때문에 일반적 신경망 학습 자체의 좋은 출발점.
- word2vec은 단어에 연관된 성질들도 학습. 예를 들어 모든 단어에 장소, 감정(긍정성), 성별 등의 성질이 다양한 정도로 연관. 말뭉치에 있는 임의의 단어가 '장소성(placeness)','인물성(peopleness)','개념성(conceptness)','여성성(femaleness)'같은 성질과 연관되어 있다면, 다른 모든 단어의 단어 벡터에도 그 성질들에 관한 일정한 점수들이 부여. word2vec이 단어 벡터들을 형성(학습)함에 따라, 한 단어의 의미가 이웃 단어들로 확산.
- word2vec은 말뭉치의 모든 단어를 단어-주제 벡터와 비슷한 수치 벡터로 표현. word2vec의 경우 '주제'들이 좀 더 구체적. LSA에서는 단어들이 같은 문서에 있기만 하면 아무리 멀리 떨어져 있더라도 의미들이 확산해 단어-주제 벡터에 반영.
- 그러나 word2vec에서는 의미가 확산하기 위해서는 단어들이 근처에 있어야 하며, 흔히 쓰이는 조건은 "같은 문장 내에서 다섯 단어 안에"있어야 함. 그리고 word2vec의  단어 벡터는 벡터 연산이 의미가 있음. 단어 벡터들을 더하고 빼면 원래의 벡터들과는 다른 의미의 단어 벡터가 나옴.
- 단어 벡터를 일단의 가중치들 또는 점수들이라고 생각하면 단어 벡터를 이해하는 데 도움이 됨. 각 가중치 또는 점수는 그 단어가 지닌 뜻의 특정 차원('성질')과 연관.