# 1.NLP 기초.
## 4. 단어 빈도에서 의미 찾기: 의미 분석
### 2. 잠재 의미 분석(LSA)
- 컴퓨터가 그런 단어 조합의 의미를 '이해'하지는 않음. 단지 그 단어들이 함께 등장할 때가 많다는 점을 알 뿐. 예를 들어 문서들에서 "dog","cat","love"가 많이 등장하면 컴퓨터는 그것들을 하나의 주제로 묶음. 그 주제가 '애완동물'에 관한 것이라는 점은 알지 못함. 사실 LSA는 'domesticated(가축화된)'과 'feral(야생화된)' 같은 상반된 단어를 하나의 주제로 묶을 수도 잇음. 그 단어들이 같은 문서에 나오는 경우가 많으면 LSA는 그 단어들에 대해 같은 종류의 주제 점수를 높게 매김. 같은 주제 점수가 높은 단어들을 살펴보고 주제 자체에 이름을 붙이는 것은 사람이 할 일.
- 주제에 이름을 붙이지 않아도 잠재 의미 분석이 가능함. 이전 장들에서 어간 추출을 거친 BOW벡터나 TF-IDF 벡터의 차원 1000개를 일일이 붙석하지는 않았듯, 우리가 주제들을 일일이 분석해서 각 주제의 '의미'를 파악할 필요가 없음. 이전에 TF-IDF 벡터들에 했던 것처럼 새 주제 벡터들에 대해 벡터 연산을 수행, 원하는 정보를 검색하면 됨.
- 의미 검색의 경우에는 단어 빈도 대신 주제 벡터들을 더하고 빼서 문서들의 유사도를 측정하는 방식으로 할 수 있음.
- LSA는 또 다른 유용한 정보를 제공. TF-IDF의 "IDF" 부분과 같이 , LSA는 벡터의 어떤 차원이 문서의 의미에 중요한지 알려줌. 문서들에 대한 분산이 가장 작은 차원(주제)은 제거해도 됨. 대부분의 기계 학습 알고리즘에서 그런 저분산 주제들은 계산을 방해하는 잡음일 때가 많음.
- 모든 문서에 비슷한 빈도로 등장하는 불용어처럼 모든 문서에서 점수가 비슷한 주제는 문서들을 구분하는 데 도움이 되지 않으며, 따라서 제거해도 무방. 그리고 그런 주제들을 제거하면 NLP 파이프라인의 일반화가 개선. 즉 이전에는 보지 못한 문서들, 다른 맥락에서 온 문서들도 파이프라인이 잘 처리.
- LSA의 이러한 일반화와 압축의 장점은 앞에서 불용어를 제거해서 얻은 장점과 유사. LSA의 차원 축소가 훨씬 효과적, LSA는 차원을 최적으로 축소, 어떤 단어도 제거하지 않고 정보를 최대한 유지,쓸데없는 차원(주제)을 폐기.
- LSA는 더 많은 의미를 더 적은 차원들로 압축. 의미 분석을 위해서는 분산이 큰 차원들, 즉 말뭉치의 문서들이 다양한 방식으로 말하는 주요 주제들만 남기면 됨. 그리고 그러한 각 차원은 해당 주제에 대한 단어의 주제 점수를 가중치로 한 일차결합. 각 차원은 해당 주제를 잘 나타내는 단어들이 무엇인지 말해줌.