# 1.NLP 기초.
## 4. 단어 빈도에서 의미 찾기: 의미 분석
### 4. 주성분 분석(PCA)
- 흔히 PCA로 줄여 쓰는 주성분 분석(Principal component analysis)는 사실 SVD의 다름 이름임. LSA에서 주제 모형의 차원을 축소하기 위해 SVD를 사용했던 것처럼 PCA도 행렬의 차원을 축소하는 데 사용.
- 그리고 scikit-learn의 PCA 모형은 NLP 파이프라인의 정확도 향상을 위해 SVD의 수학 연산을 적절히 조율한 것.
- 예를 들면 sklearn.decomposition.PCA는 자료점(예시의 경우 단어 빈도)들에서 모든 자료점의 평균을 뺌. 결과적으로 모든 자료점은 평균을 중심으로 배치. 이를 '중심화(centering)'이라고 함. 이보다는 덜 두드러지는 특징으로 . scikit-learn의 PCA 모형은 flip_sign이라는 함수를 사용, 특이 벡터의 부호를 결정론적으로 계산.
- scikit-learn의 PCA 모형은 '백화(whitening)' 기능도 제공. 백화는 단어-문서 벡터를 주제-문서 벡터로 변환할 때 특잇값들을 무시하는 것과 비슷한 요령. 단, 앞에서는 $S$의 대각성분(특잇값)을 모두 1로 설정, 백화 기능은 sklearn.StandardScaler 변환이 했던 것처럼 자료를 분산 값들을 이용해서 분리. 이렇게 하면 자료가 좀 더 넓게 퍼져서 최적화 알고리즘이 자료 공간의 '하프 파이프(half pipe)' 또는 '계곡'에서 벗어나지 못하는 문제가 감소. 자료 집합의 특징들이 서로 상관관계가 높을 때 위와 같은 결과가 발생할 수 있음.
- PCA를 실제의 고차원NLP 자료에 적용하는 방법으로 넘어가기 전에 잠시 PCA와 SVD의 효과를 눈으로 확인. 3차원 자료점들에 대해 PCA를 적용, 그것을 3차원 그래프로 표시. 이 시각화 예시는 scikit-learn의 PCA 구현에 대한 API를 이해하는 데도 도움이 됨. PCA는 다양한 용도로 사용, PCA를 잘 이해하면 NLP 이외의 분야에서도 도움.
- 대부분의 '진짜'NLP 문제에서는 scikit-learn의 PCA 모형(sklearn.decomposition.PCA)을 잠재 의미 분석에 사용. 단 RAM에 모두 담을 수 없을 정도로 말뭉치가 클 때는 대안 필요
- 위 경우에는 scikit-learn의 IncrementalPCA 모형을 사용 혹은 뒤에서 이야기하는 규모가변성 개선 기법을 사용.