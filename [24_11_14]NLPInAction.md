# 1.NLP 기초.
## 4. 단어 빈도에서 의미 찾기: 의미 분석
### 1. 단어 빈도에서 주제 점수로
#### 4. 주제 점수를 매기는 알고리즘
- 큰 수치 행렬에 대해 차원 축소 기법을 적용하는 예시로, 디지털 이미지는 픽셀 색상 값들로 이루어진 행렬. 이미지나 그와 비슷한 고차원 자료에 대한 기계 학습을 공부해 봤다면 주성분 분석(principal component analysis, PCA)라는 기법을 학습. PCA에 적용되는 수학은 LSA와 정확히 같음. PCA는 이미지나 기타 수치 행렬의 차원을 줄이기 위한 것이고 LSA는 BOW 벡터나 TF-IDF 벡터들의 차원을 줄이기 위한 것이라는 점이 다를 뿐
- PCA 기법을 단어들의 의미 분석에 사용할 수 있다는 점을 연구자들이 발견한 것은 비교적 최근의 일. 연구자들은 새로운 용도를 반영해 기법에 LSA라는 개별적 명칭을 부여. scikit-learn의 '적합 및 변환'(fit and transform)기능을 이용해 문서의 의미를 나타내는 벡터를 계산. 비록 scikit-learn은 이를 PCA 모형이라고 불러도 이름이 다를 뿐 LSA와 같음.
- LSA와 이름이 다를 뿐 사실상 같은 기법은 이외에도 존재. 전문 검색(full text search)을 위한 색인 생성에 초점을 두는 정보 검색 분야에서는 LSA를 LSI(latent semantic indexing;잠재 의미 색인화)라고 부를 때 많음. 그러나 LSI 알고리즘 자채는 색인을 생성하지 않기 때문에 오해 소지가 있음. LSI 알고리즘이 산출하는 주제 벡터는 완벽하게 색인화할 수 없을 정도로 차원이 큼.
##### LSA와 유사한 알고리즘들
- LSA와 이름뿐만 아니라 NLP에서 용도도 비슷한 알고리즘이 둘 존재.
  - LDA(linear discriminant analysis;선형 판별 분석)
  - LDiA(latent Dirichlet allocation; 잠재 디클레 할당)
- LDA는 하나의 문서를 단 하나의 주제로 축약. LDiA는 문서들을 원하는 만큼의 여러 주제로 축약할 수 있다는 점에서 LSA와 좀 더 유사.
  - LDA는 1차원이므로 특잇값 분해(SVD)가 필요하지 않음. 그냥 한 부류의 훈령용 TF-IDF 벡터들의 무게중심과 다른 부류의 훈련용 TF-IDF 벡터들의 무게중심을 각각 계산. 그 두 무게중심을 잇는 직선을 하나의 1차원 축으로 간주, 새 TF-IDF 벡터들을 그 축에 투영하면 TF-IDF 벡터들을 두 부류로 분류 가능. 
- 알고리즘 중 가장 간단한 LDA를 이용, 주제 분석을 수행. 좀 더 복잡한 LSA와 LDiA를 시험.