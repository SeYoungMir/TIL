# 1.NLP 기초.
## 3. TF-IDF 벡터
### 4. 주제 모형화
#### 1. IDF와 지프의 법칙
- 문서1백만 건을 이루어진 말뭉치에서 누군가가 "cat"이라는 단어로 말뭉치를 검색했다고 가정. 만일 1백만 분서 중 "cat"이 있는 문서가 딱 하나라면 "cat"의 IDF는 1000000/1 = 1000000(백만)
- "dog"이라는 단어가 있는 문서가 10건이면 "dog"의 IDF는 1000000/10 = 100000
- 이는 큰 차이. 지프의 법칙에 따르면 두 단어의 순위가 멀 수록 빈도의 차이는 지수적으로 차이남.
- 지프의 법칙을 적용할 때는 용어 빈도에 로그를 씌워서 수치의 규모를 줄일 때가 많음. 로그 함수(log())는 지수함수,즉 거듭제곱(exp())의 역
- 이는 IDF에도 도움이 됨. "cat"과 "dog"의 예에서, 말뭉치가 크면 두 단어의 출현 횟수 차이가 그리 크지 않더라도 IDF의 차이는 아주 클 수 있음. IDF에 로그를 씌우면 두 용어의 IDF를 비교하기 좋을 뿐만 아니라, 전체적인 TF-IDF 점수들이 좀 더 고르게 분포됨. 따라서 IDF를 그냥 전체 문서 수 대 단어 출현 문서 수의 비율('확률')이 아니라 그 비율의 로그로 정의, TF-IDF 점수의 계산을 위해 TF에도 로그를적용.
- 수치들을 어떤 특정 범위로 변환하려는 것이 아니므로, 로그의 밑(기수)은 중요하지 않음. 어떤 밑을 사용하든, 로그 덕분에 빈도 분포가 균일해지기만 하면 됨. 밑을 10으로 하는 상용로그를 주로 사용.
- 이렇게각 문서의 TF들을 말뭉치(또는 해당 언어 전체)에서의 전반적인 쓰임새를 고려해서 고찰 가능.
- 정리하자면 주어진 말뭉치 $D$의 임의의 문서 $d$와 임의의 단어 $t$에 대한 TF와 IDF,TF-IDF점수의 정의는 다음과 같음.
  - $tf(t,d) = \frac{count(t)}{count(d)}$
  - $idf(t,D)= log\frac{전체 문서 수}{t가 나오는 문서 수}$
  - $tfidf(t,d,D)=tf(t,d)*idf(t,D)$
