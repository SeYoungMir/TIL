# 1.NLP 기초.
## 4. 단어 빈도에서 의미 찾기: 의미 분석
### 6. 거리와 유사도
- 새 주제 벡터 공간으로 문서를 분류하려면 제 2장과 제 3장에서 이야기한 유사도 점수를 다시 살펴볼 필요가 있음, 문서르르 표현하는 벡터들 사이의 거리 또는 유사도 점수를 비교함으로써 두 문서가 어느정도나 비슷한지 파악할 수 있음.
- LSA 주제 모형이 제 3장의 고차원 TF-IDF 벡터 모형과 어느정도나 일치하는지를 유사도 점수를 이용해서 파악. 더 높은 차원의 단어 모음들에 담긴 정보를 대거 제거한 모형의 벡터들이 고차원 벡터들과 여전히 비슷하다면 그 모형은 좋은 모형이라고 할 수 있음. 또한, 두 주제 벡터 사이의 거리 또는 각도는 해당 문서들의 주제 또는 의미가 얼마나 비슷한지 말해 줌. 좋은 주제 모형이라면 비슷한 주제의 문서들에 대한 벡터들이 주제 벡터 공간 안에서 서로 가까이 있어야 함.
- LSA는 벡터들 사이의 큰 거리를 유지(즉, 원래 떨어져 있던 벡터들은 차원 축소 이후에도 떨어져 있음),가까운 거리를 항상 유지하지는 않음. 즉, LSA를 거치면서 문서들 사이의 미세한 관계'구조'가 소실될 수 있음. 이는 LSA에 깔린 SVD 알고리즘이 새 주제 벡터 공간에서 모든 문서의 분산을 최대화하는 것에 초점.
- 특징 벡터(단어 벡터, 주제 벡터, 문서 문맥 벡터 등등)들 사이의 거리는 NLP 파이프라인의(그리고 거의 모든 기계 학습 파이프라인의) 성과에 큰 영향. 고차원 공간에서 거리를 측정하는 방법은 아주 다양. 따라서 주어진 NLP 문제에 가장 잘 맞는 측정 방법을 고를 수 있어야 함. 다음은 흔히 쓰이는 거리 측정 방법, 선형 대수나 기하학 과정에서 이미 배운 것도 있지만 처음 보는 것도 존재.
  - 유클리드 거리 또는 평균 제곱근 오차(root mean square error,RMSE):2-노름($L_2$)
  - 제곱 유클리드 거리 또는 제곱합 거리(sum of squares distance, SSD) :$L^2_2$
  - 코사인 거리 또는 각 거리(angular distance)또는 투영 거리: 정규화된 내적
  - 민코프스키 거리(Minkowski distance): p-노름 ($L_p$)
  - 분수 거리(fractional distancce)또는 분수 노름: 0<p<1 인 p 노름($L_p)$
  - 시가지 거리, 맨해튼 거리, 택시 거리, 절대 거리 합(sum of absolute distance, SAD): 1-노름($L_1$)
  - 자카르 거리(Jaccard distance), 역 집합 유사도(inverse set similarity)
  - 마할라노비스 거리(Mahalanobis distance)
  - 레벤시테인 거리 또는 편집 거리(edit distance)