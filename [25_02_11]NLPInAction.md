# 2. 더 깊은 학습: 신경망 적용
## 5. 신경망 첫걸음: 퍼셉트론과 역전파
### 1. 신경망의 구성 요소
#### 8. 심화 학습
- 심층(다층) 신경망이 기계 학습 분야에 퍼지면서 신경망의 세부 사항에 관해 많은 연구가 진행. 주요 연구 주제들은 다음과 같음
  - 다양한 활성화 함수들(S자형 함수, 정류 선형 단위, 쌍곡탄젠트 등)의 장단점
  - 오차의 반영 정도를 결정하는 학습 속도를 잘 선택하는 방법
  - 운동량(momentum)을 이용한 학습 속도의 동적 조율(최소점을 더 빨리 찾기 위한)
  - 드롭아웃(dropout)기법. 주어진 한 훈련 패스에서 가중치 중 일부를 무작위로 선택, 탈락(드롭아웃)시킴으로써 모형이 훈련 집합에 과대적합하는 일을 방지.
  - 한 가중치가 다른 가중치들보다 너무 많이 커지거나 작아지지 않도록 인위적으로 제한을 가하는 정칙화(regularization)기법들. 과대적합을 방지하는 것이 목적.

#### 9.정규화:스타일 있는 입력
- 신경망은 주어진 입력 벡터들로부터 최대한 많은 것을 배우려 함. 신경망의 학습을 도우려면 입력을 적절히 정규화할 필요가 있음. 심층 학습뿐만 아니라 다른 여러 기계 학습 모형에서도 정규화가 중요.
- 주택이 특정 시장에서 팔릴지 아닐지를 신경망으로 예측. 간결함을 위해 입력 특징은 침실 수와 판매가 두 가지로 압축. 다음은 침실이 두 개이고 판매가가 $275,000인 주택을 서술하는 입력 벡터
- ```python
  input_vec = [2,275000]
  ```
- 두 수치의 규모가 상당히 다름에 주목. 역전파 과정에서 침실 수와 연관된 가중치는 판매가와 연관된 가중치의 규모때문에 필요 이상으로 크게 변화. 이런 문제를 피하기 위해 원래의 수치에 담긴 정보(특히 견본 간 차이에 관한 정보)를 유ㅜ지하면서도 수치들을 역전파 알고리즘에 적합한 형태로 정규화하는 기법이 흔히 사용. 
- 한 벡터의 여러 성분이 대체로 비슷한 범위의 값이 되도록 정규화 한다면 앞에서 말한 문제가 해결. 구체적인 정규화접근 방식은 평균 정규화, 특징 비례, 계수 변동 등으로 다양하지만, 이들은 모두 한 견본의 모든 성분(특징)이 [-1,1]이나 [0,1]같은 일정한 범위를 넘지 않게 하는 것, 그러면서도 원래의 정보를 잃지 않는 것을 목표로 함.
- NLP에서는 정규화 문제를 그리 걱정할 필요가 없음. TF-IDF나 원핫 부호화, word2vec은 이미 정규화된 자료이기 때문. 그렇지만 원본 단어 빈도 같은 정규화되지 않은 특징들을 다룰 일이 있다면 정규화에 신경을 써야 함.