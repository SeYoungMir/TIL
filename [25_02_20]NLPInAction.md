# 2. 더 깊은 학습: 신경망 적용
## 6. 단어 벡터를 이용한 추론: word2vec 활용
### 2. 단어 벡터
#### 1. 벡터 지향적 추론
- word2vec이 대중에게 처음 공개된 것은 2013년 ACL(Association for Computational Linguistics)컨퍼런스에서였음. "Linguistic Regularities in Continuous Space Word Representations."라는 제목의 논문이 놀랄 만큼 정확한 언어 모형을 설명. 앞의 예들과 비슷한 비유 질문들에 대한 그 모형(word2vec 내장)의 정확도가 45%로 이는 해당 LSA 모형의 11%의 약 4배 정확도를 보임. 그런 정확도 증가가 너무 놀라운 탓에 ICLR(International Conference on Learning Representations)은 미클로프의 초기 논문 게재를 거부. 논문 검토자들이 믿지 못할 정도로 모형의 성과가 좋았기 때문.
- 그로부터 거의 1년이 지나 연구팀이 소스 코드를 공개, ACL이 논문을 게재.
- 단어 벡터가 등장하면서, 포틀랜드 팀버스에 해당하는 시애틀의 축구팀을 묻는 문제를 다음처럼 벡터 연산으로 계산 가능.
  - ```python
    Portland Timbers + Seattle - Portland = ?
    ```
- 다음 그림은 위 연산을 도식화.
  - ![alt text](image-14.png)
- word2vec 모형은 단어들 사이의 관계에 대한 정보를 담으며, 여기에는 유사성에 관한 정보도 포함. word2vec 모형은 Portland라는 용어와 Portland Timbers라는 용어가 Seattle과 Seattle Sounders만큼이나 가깝다는(비슷하다는)점을 "앎". 서로 짝지어진 이 벡터들은 거리가 가깝고 방향도 거의 같음. word2vec 모형이 스포츠팀에 관한 비유 질문에 답할 수 있는 것은 이런 벡터 연산 덕분임. Portland 벡터와 Seattle 벡터의 차이를 Portland Timbers 벡터에 더하면 Seattle Sounders 벡터에 가까운 벡터가 도출.
- 단어 벡터들을 더하고 빼고 나면 어휘의 단어 벡터 중 하나와 거의 같은 벡터가 나옴. 일반적으로 word2vec 모형의 단어 벡터들은 차원 수가 100 정도. 그리고 각 차원의 성분은 연속 실숫값. 어쨌든 중요한 것은 NLP 질문을 표현한 벡터 연산의 결과는 어휘의 어떤 한 단어 벡터와 거의 비슷한 벡터라는 점. 그 벡터 근처에 있는 한 단어가 바로 NLP 질문에 대한 자연어 답임.
- 정리하자면, word2vec을 이용하면 토큰 출현 횟수와 빈도들로 이루어진 자연어 벡터들을 그보다 훨씬 낮은 단어 벡터 공간으로 변환 가능. 그리고 그러한 저차원 공간에서 벡터들에 통상적인 선형대수 벡터 연산들을 적용해서 새로운 벡터를 얻고, 그 벡터를 다시 자연어 공간으로 변환함으로써 자연어 추론 문제를 해결 가능. 이러한 능력이 챗봇이나 검색 엔진, 질의응답 시스템, 정보 추출 알고리즘에 유용.
- 단어 벡터들을 연구하면서 word2vec 연구팀은 한 단어의 단수형과 복수형의 벡터 차이가 대부분의 단어에서 비슷하다는 점에 주목.
- 이러한 유사성은 단수형과 복수형에만 한정되지 않음. 연구팀은 다른 여러 의미 관계에서도 이런 유사성을 발견, 급기야는 문화, 인구 등에 관환 질문의 답을 벡터 연산으로 구할 수 있음을 알 수 있었음.