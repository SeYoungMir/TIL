# 1.NLP 기초.
## 1. 사고의 단위: NLP 개요
### 5. 벡터 차원 축소 - 이어서
- 단어 모음 벡터와는 달리 이러한 평가치 벡터들은 실제로 프로그램에서 다룰 수 있음.
- 더 나아가서, 무장들을 좀 더 군집화해서 벡터들을 더욱 단순화, 일반화하는 것도 가능.
- 여기서 군집화(clustering)은 일부 차원(축)들에 대해 문장들을 좀 더 가깝게 만드는 것을 말함.
- 그런데 컴퓨터가 이런 벡터의 각 차원에 평가치를 부여하게 하려면 어떻게 해야하는가. 한 가지 방법은 문장에 good 이라는 단어와 morning이라는 단어가 있는지 같은 간단한 질문들로 문장을 평가, 각 질문의 답을 비트 0 또는 1로 표현해서 하나의 "원 핫 부호화"벡터(one-hot encoded vector), 즉 원핫 벡터를 만드는 것임.
- 이런 원핫 벡터들로 이루어진 모형이 첫 번째 실용적 벡터 공간 모형인데, 이를 비트 벡터 언어 모형이라고 부름. 컴퓨터로 자연어를 잘 처리할 수 있는 이유는 이 때문이며, 80년대에는 심지어 슈퍼컴퓨터로도 수백만, 수천만 차원의 벡터들을 계산하기가 거의 불가능했지만, 21세기에는 소비자용 노트북으로도 가능하다.
- NLP의 현실화에는 연산 장치의 속도 향상 뿐만 아니라 메모리 및 저장 장치의 용량 증대도 일조했고, 선형 대수 알고리즘들의 발전도 컴퓨터로 자연어를 처리하는 난제를 해결하는 데 결정적인 역할을 함.
- 챗봇에 이보다 더 간단한, 더 큰 표현을 사용할 수도 있음. 각각의 문자를 개별 차원으로 두는 것인데, 그런 벡터는 "첫 글자가 A인가, B인가?.. 둘째 글자가 A인가? ..."로 이어지는 질문의 답들로 구성. 이런 벡터에는 문자들과 단어들의 정확한 순서를 포함해서 원문의 모든 정보가 고스란히 담겨 있다는 장점이 있음.
- 이러한 표현은 원래 문서의 ASCII 부호 표현보다 길며, 가능한 문서 표현의 개수가 너무 많다는 단점이 원문의 정보를 유지한다는 장점을 훨씬 능가.
- 문자들과 단어들의 순서를 유지하는 것은 좋으나, NLP 문제의 차원 수 증대.
- 이런 문자별 벡터 공간에서는 문서 표현들이 잘 뭉치지 않아 군집화가 잘 안되며, 러시아 수학자 블라디미르 레벤시테인이 두 문자열의 유사성을 빠르게 찾아내는 재치 있는 접근 방식을 고안함.
- 이 알고리즘 덕분에 단순하고 기계적인 언어모형으로도 놀랄 만큼 유용한 챗봇 몇 개를 만들 수 있는데, 이는 개별 문자나 토큰으로 이루어진 날카로운 고차원 공간을 좀 더 희미한 의미를 가진 주제 벡터들의 저차원 벡터로 압축 또는 내장할 때 유용.
- 잠재적 의미 색인화와 잠재적 디리클레 할당이 이에 해당.