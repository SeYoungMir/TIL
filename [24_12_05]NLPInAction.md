# 1.NLP 기초.
## 4. 단어 빈도에서 의미 찾기: 의미 분석
### 4. 주성분 분석(PCA)
#### 2. NLP에서의 PCA
- 표본이 햄 쪽으로 편향된 현상이 마음에 들지 않는다면, 햄을 정확하게 분류했을 때 더 보상을 제공하도록 모형을 조율 가능. 어휘 크기 $|V|$가 클 때는 그렇게 하기 쉽지 않음.
- 예시에서 어휘 크기는 9,232고 말뭉치 크기(메시지 수)는 4,837. 즉, 메시지들에 쓰인 고유한 토큰의 수가 메시지 수의 두 배 정도이며, 이 문자 메시지들에서 스팸으로 분류된 것은 딘 683개(전체 메시지의 약 8분의 1)밖에 되지 않음. 이는 어휘에서 '스팸성'단어가 상대적으로 적다는 뜻이며, 따라서 과대적합(overfitting;또는 과적합)이 발생하기가 아주 쉬움.
- 스팸 필터링의 맥락에서 과대적합은 스팸 필터가 적은 수의 익숙한 단어들에만 의존하는 것을 말함. 스팸 필터는 스팸 메시지에 흔히 쓰이는 단어, 즉 ;스팸성;단어들에 근거해서 메시지를 분류. 스패머는 그런 스팸성 단어의 동의어를 사용, 스팸 필터를 무력화하려 함. 만일 어휘에 스패머의 새로운 동의어들이 포함되어 있지 않으면, 스팸 필터는 그런 교묘하게 작성된 문자 메시지를 스팸이 아니라 햄이라고 오분류.
- 이런 과대적합은 NLP의 고질적인 문제. 사람들의 다양한 어법과 어휘를 포함하며 응용 분야에 맞는 적절한 분류명이 붙은 자연어 자료 집합을 구하긴 쉽지 않음. 그런 자료 집합을 만들어 낼 수 있는 기업은 극히 소수. 따라서 우리가 할 수 있는 일은 과대적합을 기정사실로 받아들이고 그에 대한 대응책을 모색하는 것 뿐. 즉, 적은 수의 훈련 견본으로 모형을 훈련해도 모형이 미지의 견본들에 잘 "일반화되게"하는 알고리즘을 찾아야 함.
- 차우너 축소는 과대적합에 대한 주된 대응책. 수많은 단어를 그보다 적은 수의 주제들로 통합함으로써 차원이 축소, 결과적으로 NLP 파이프라인이 좀 더 '일반화'. 예제의 경우 어휘의 고유한 단어들을 더 적은 수의 주제로 줄여서 차원을 축소하면 스팸 필터가 좀 더 다양한 문자 메시지를 처리 가능.
- LSA가 하는 일이 그런 것. LSA는 차원을 축소, 따라서 과대적합을 방지. LSA는 단어 출현 횟수들 사이에 어떠한 선형(일차)관계가 존재한다는 가정하에서 작은 자료 집ㅎㅂ으로 모형을 일반화. 예를 들어 광고 스팸 메시지들에는 "half" 와 "off"가 함께 나오는 경우가 많음(절반 가격을 뜻하는 "Half off!")
- LSA는 단어들 사이의 그런 관계를 식별, 그런 조합이 얼마나 스팸에 가까운지 측정. 이에 기초해서 모형이 "80% off"같은 문구도 스팸성으로 분류하도록 일반화 가능. NLP 훈련 자료 집합에 "discount"와 "off"가 자주 등장하는 스팸 메시지가 많다면 "80% discount"같은 문구도 일반화.
  - 일반화가 기계 학습과 인공지능 분야 전체의 핵심 난제로도 생각. 일반화에 필요한 훈련 자료의 양을 줄이는 기법으로 '단발 학습(one-shot learning)'이 있음. 이 기법을 이용 시 전통적인 모형에 쓰이는 자료 집합보다 몇 자릿수 적은 크기의 자료로도 전통적인 모형과 비슷한 정확도를 달성하는 것이 가능할 수 있음.
- NLP 파이프라인이 주어진 훈련 자료 집합의 문자 메시지들 뿐만이 아닌 실제 세상의 좀 더 다양한 문자 메시지들도 처리할 수 있으려면 이러한 일반화가 필요.