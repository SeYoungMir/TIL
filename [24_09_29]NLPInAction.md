# 1.NLP 기초.
## 2. 단어 토큰화
### 2. 토큰 생성기를 이용한 어휘 구축
#### 4. n-그램을 이용한 어휘 확장
- 불용어
  - 불용어 또는 정지 단어(stop word)는 아주 자주 출현하지만 문구의 의미에 관한 실질적인 의미는 별로 담고 있지 않은 단어를 말함.
  - 어떤 언어에나 불용어가 있음. 다음은 영어의 주요 불용어.
    - a, an
    - the, this
    - and, or
    - of, on
  - 예전부터 NLP 파이프라인에서는 텍스트의 정보 추출에 필요한 계산량을 줄이기 위해 불용어를 제외. 그러나 불용어 자체는 정보를 별로 전달하지 않는다고 해도, n-그램에 포함된 불용어는 중요한 관계 정보를 제공할 수 있음. 다음 두 예를 참고
    - Mark reported to the CEO
    - Suzanne reported as the CEO to the board
  - 이들을 4-그램 단위로 토큰화 하면 reported to the CEO(CEO에게 보고),와 reported as the CEO(CEO로서 보고)같은 4-그램들이 만들어짐. 그런데 만일 전치사나 정관사같은 불용어를 모두 제거 시 둘 다 그냥 "reported CEO"로 회사 내의 위계 구조에 관한 정보가 사라짐.
  - 첫 문장에서 Mark는 CEO의 부하 직원, 둘째 문장에서 Suzanne은 CEO라는 차이가 있음.이처럼 종종 불용어들이 유용한 정보를 제공. 그러나 불용어들을 이런 식으로 활용하려면 n-그램이 더 길어야 한다는 단점이 있음. 이 예제 문장들에서 불용어가 유용하려면 n이 적어도 4는 되어야함.
  - 불용어 필터의 구체적 설계는 응용프로그램에 따라 다름. NLP 파이프라인에서 토큰화 단계가 산출하는 어휘의 크리는 이후 모든 단계의 계산 복잡도와 메모리 요구량에 큰 영향.
  - 불용어들은 전체 어휘의 극히 일부. 일반적 말뭉치에서, 자주 등장하지만 별로 중요하지 않은 불용어들은 100개 정도, 그러나 대량의 트윗, 블로그 글, 뉴스 기사 모음에 등장하는 단어들의 약 95%를 포괄하는 데 필요한 어휘의 크기는 약 2만 단어, 이도 1-그램(한 단어)의 토큰만 고려했을 때 이야기.
  - 대형 영어 말뭉치의 2-그램들의 95%를 포괄하려면 1백만 개 이상의 고유한 2-그램 토큰들로 이루어진 어휘 필요.