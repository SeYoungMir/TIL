# 2. 더 깊은 학습: 신경망 적용
## 5. 신경망 첫걸음: 퍼셉트론과 역전파
### 1. 신경망의 구성 요소
#### 3. 치우침 단위
##### 역전파
- 힌턴과 동료들은 하나의 목표변수에 대해 여러 개의 퍼세브론을 동시에 사용하는 것이 해결책임을 확신하고 연구를 진행. 그들은 다수의 퍼셉트론을 함께 사용한다면 선형 분리 가능이 아닌 문제도 풀 수 있음을, 즉 비선형 함수들도 선형 함수들처럼 퍼셉트론들로 근사할 수 있음을 보임.
- 그런데 하나가 아닌 여러 개의 퍼셉트론의 가중치들을 어떻게 갱신해야할까? 그리고 오차를 가중치들에 배분한다는 것이 과연 무슨 뜻일가? 병렬로 배치된 두 퍼셉트론(뉴런)에 같은 자료점을 입력한다고 하자. 두 퍼셉트론의 출력으로 무엇을 하든(둘을 합치든, 연결하든, 곱하든), 그 오차를 다시 원래의 가중치들로 밀어넣는 것은 입력의 함수. 그리고 두 퍼셉트론의 입력은 동일. 따라서 훈련 과정의 매 반복에서 두 퍼셉트론은 같은 정도로 갱신. 둘의 가중치는 결국 같아지므로 퍼셉트론을 하나만 사용하는 것과 마찬가지, 따라서 신경망은 별로 배우는 것이 없음.
- 그런데 두 퍼셉트론을 직렬로 연결해서 한 퍼셉트론의 출력이 다른 퍼셉트론에 입력되게 하면 흥미로운 일이 생김. XOR 문제를 위해 우리가 하려는 일도 그것.
- 역전파는 단일 퍼셉트론의 한계를 극복하는 데 도움을 주지만, 역전파를 활용하려면 퍼셉트론의 구조를 좀 더 조율해야 함. 다중 퍼셉트론에서 가중치들은 그것이 전체 오차에 얼마나 기여하느냐에 기초해서 갱신. 그런데 한 퍼셉트론의 출력이 다른 퍼셉트론의 입력이 되면, 둘째 퍼셉트론의 관점에서 보는 오차의 정의가 명확하지 않음.
- 이 문제를 해결하려면 한 층의 한 가중치가 오차에 기여하는 정도를 그 다음 층에 있는 다른 가중치들에 근거해서 계산하는 방법이 필요. 그리고 역전파가 바로 그러한 방법.
- 이제는 원래의 단일 퍼셉트론에서 많이 멀어졌으므로, 신경망을 구성하는 기본 계산 단위를 좀 더 일반적인 용어인 뉴런이라고 부르기로 함. 요즘 신경망들에 쓰이는 뉴런의 구조는 다양, 원래의 퍼셉트론은 그 중 하나일 뿐임. 다른 심층 학습 문헌들에서는 뉴런을 노드나 단위(unit),세포(cell)라고 부르기도 하는데, 많은 경우 이들은 서로 바꿔 써도 뜻이 통함.