# 1.NLP 기초.
## 4. 단어 빈도에서 의미 찾기: 의미 분석
### 5. 잠재 디리클레 할당(LDiA)
#### 3.LDiA +LDA = 스팸 분류기
- LDiA의 주제들이 스팸성 예측 같은 뭔가 유용한 예측에 얼마나 유용한지 탐색. 이를 위해 앞의 LDiA 주제 벡터들로 LDA 모형을 다시 훈련(PCA 주제 벡터들로 했던 것과 동일)
- ```python
  from sklearn.discriminant_Analysis import LinearDiscriminantAnalysis as LDA
  X_train,X_test,Y_train,Y_test = train_test_split(ldia16_topic_vectors,sms.spam,test_size=0.5,random_state=271828)
  lda = LDA(n_components=1)
  lda = lda.fit(X_train,Y_train)
  sms['ldia16_spam']=lda.predict(ldia16_topic_vectors)
  rount(float(lda.score(X_test,Y_test)),2)
  ```
  - train_test_split()과 LDiA의 알고리즘을 확률적. 따라서 이 예제를 실행할 때마다 다른 결과와 다른 정확도 수치가 출력. 실험의 재현을 위해 파이프라인이 매번 같은 결과를 내게 하고 싶다면, 이 모형들과 자료 집합 분리 함수의 seed 인수를 살펴봐야 함. 종잣값을 매번 같게 하면 재현 가능한 결과를 획득 가능.
  - 텍스트에 항상 같이 쓰이고 혼자 쓰이지는 않는 단어들로 이루어진 2-그램이나 3-그램이 많이 있으면'colinear(동일 선상)' 경고가 발생. 그런 경우 LDiA 모형은 그런 동등한 단어 빈도들의 가중치를 임의로 분리. 문자 메시지들에서 이런 동일선상(판별식이 0)경고를 발생하는 사례들이 있는지 탐색. 한 단어가 항상 다른 단어와 함께 쓰이는 경우를 보면 됨.
  - 물론 그런 사례들을 사람이 일일이 찾기보다는 파이썬으로 검색하는 것이 나음. 먼저 말뭉치의 단어 모음 벡터들에서 서로 같은 것이 있는지 봐야 함. 예를 들어 "Hi there Bob!" 과 "Bob, Hi there"은 단어 순서는 다르지만 단어 빈도들이 같으므로 단어 모음 벡터도 같은. 파이썬으로 모든 단어 모음 벡터를 다른 모든 단어 모음 벡터와 비교하면 이런 사례를 찾을 수 있음.LDiA이든 LSA이든 이런 사례들은 반드시 '동일 선상' 경고를 발생.
  - BOW 벡터들에서 중복을 찾지 못했다면, 어휘의 모든 가능한 단어 쌍을 나열하고 각각을 단어 모음들과 비교해서 해당 단어 쌍의 두 단어가 있는 문자 메시지들을 찾아봐야 함. 만일 그 두 단어가 항상 같은 메시지에 등장하고, 각 단어가 따로 쓰이는 문자 메시지가 없다면 그 두 단어는 '동일 선상' 경고를 발생. 이런 문제를 발생하는 2-그램의 흔한 예로 유명인사의 성명(성과 이름) 이 있음. 예를 들어 문자 메시지들에 항상 "Elon Musk"만 나오고 "Elon"과 "Musk"가 따로 나오는 메시지가 없다면 이 이름은 동일 선상 경고의 원인이 됨.