# 1.NLP 기초.
## 3. TF-IDF 벡터
### 2. 벡터화
#### 1. 벡터 공간
- 자연어 문서의 벡터 공간에서 벡터 공간의 차원 수는 전체 말뭉치에 등장하는 고유한(중복되지 않는) 단어들의 개수와 동일. TF(용어 빈도)벡터나 TF-IDF 벡터를 다룰 때는 이 고유 단어 수를 대문자 "K"로 표기할 때가 많음. 그리고 이 고유 단어 수는 말뭉치의 어휘(vocabulary)크기이기도 하므로 학술 논문에서는 이를 흔히 "|V|"로 표기.
- 정리하자면, 말뭉치의 모든 문서는 K차원 벡터 공간의 K차원 벡터들로 표현. 앞의 예제인 3문서 말뭉치의 경우에 K= 18임.
- 일반적으로 사람이 시각적으로 이해할 수 있는 공간은 3차원까지이고, 3차원조차도 지면(2차원 종이)에 표현하기 쉽지 않음. 여기서는 2차원 벡터만 고려.
- 더 높은 차원이라도 이하의 내용이 기본적으로 동일하게 적용, 단지 차원이 높으면 시각화하기 어려울 뿐.
- 문서를 하나의 공간을 기준으로 한 벡터로 표현하면 문서들의 유사도를 수치로 계산 가능. 직교 좌표계에서 두 벡터의 거리는 한 벡터에서 다른 한 벡터를 빼서 만든 벡터의 길이.
- 두 벡터의 거리는 한 벡터의 머리(화살표 끝)에서 다른 벡터의 머리까지의 거리이며, 이런 거리를 2-노름(norm)거리라고 부름. 단어 출현 횟수 벡터, 즉 용어 빈도(TF)벡터들은 이런 식으로 거리를 측정하는 것이 바람직하지 않음.
- 서로 가까운(거리가 짧은) 두 벡터는 서로 "비슷하다"고 할 수 있음. 두 문서의 벡터 표현이 비슷하면 두 문서는 비슷함.
- 그런데 벡터 거리의 정의에 의해. 두 벡터가 가리키는 방향이 비슷할수록, 그리고 두 벡터의 길이가 비슷할 수록 두 벡터의 거리가 감소. 비슷한 길이의 두 문서에서 얻은 용어 비도 벡터들은 길이가 비슷, 따라서 두 벡터의 거리가 짧음. 문제는 두 문서의 길이가 같다고 해서 그 두 문서가 비슷하다는 것은 아님. 두 문서가 비슷한 단어들을 비슷한 빈도로 사용했는지 측정하는 것이 합리적. 두 문서가 비슷한 정보 내용을 담고 있는지 추정을 하기에는 이 방법이 나음.
- 거리 대신 각도에 기초해 두 벡터의 유사도 측정 가능. 두 벡터의 사이의 각도 $\theta$ 의 코사인을 표현한 유사성 측도를 코사인 유사도(cosine similarity)라고 부름.
- 이 코사인 유사도는 유클리드 내적 공식에서 유도
  - $A \cdot B = |A| |B| \times cos\theta$
- 이를 코사인 유사도에 대해 정리하면 다음과 같음.
  - $cos\theta = \frac{A \cdot B}{|A||B|}$
- 이 공식을 이용하면 삼각함수를 전혀 사용하지 않고 코사인 유사도를 효율적으로 계산 가능. 코사인 유사도는 -1에서 1까지로, 대부분의 기계 학습 문제에서 편하게 다룰 수 있는 범위.