# 1.NLP 기초.
## 2. 단어 토큰화
### 2. 토큰 생성기를 이용한 어휘 구축
#### 4. n-그램을 이용한 어휘 확장
- 이번 장 도입부에서 언급한 "ice cream"  문제를 다시 생각해보자. 이런 문장이 있다.
  - I scream, you scream, we all scream for ice cream
- 위 문장에서 우리 모두가 소리 높여 요구하는(scream) 것은 "ice"나 "cream"이 아닌 "ice cream"임. 따라서 이 문장을 표현하는 단어 벡터에서 "ice" 와 "cream"이 하나의 단위로 지정되게 하는 방법을 찾아야 함.
- 해결책은 n-그램이 될 수 있음
    - n-그램은 어떤 요소들이 순차열(보토의 경우, 문자열)에서 추출한 최대 n개의 요소로 이루어진 순차열임. NLP에서 '요소'는 텍스트를 구성하는 문자나 음절, 단어이지만, 응용에 따라서는 DNA 염기 서열을 표현하는 데 쓰이는 "A","T","G","C"같은 기호일 수도 있음.
    - 여기에서는 문자가 아니라 단어를 요소로 하는 n-그램만 다룸. 따라서 여기에서 2-그램은 "ice cream"과 같은 두 단어의 쌍, 3-그램은 "beyond the pale"이나 "Johann Sebastian Bach", "riddle me this"같은 세 단어 쌍.
    - n-그램의 단어들이 반드시 어떤 특별한 의미를 가지는 것은 아님. 즉 , 복합어가 아닌 단어 조합도 n-그램이 될 수 있음. n-그램은 그냥 일련의 요소 중 인접한 몇 개의 요소들임.
    - n-그램의 필요성은 토큰들을 하나의 단어 모음 벡터로 표현하면 단어들의 순서에 담긴 정보가 사라지기 때문.
    - 단어 하나로 된 토큰이라는 개념을 여러 단어로 이루어진 토큰인 n-그램으로 확장 시 NLP 파이프라인은 문장의 단어 순서에 담긴 의미를 좀 더 많이 유지 가능.
    - 예를 들면 뭔가의 의미를 뒤집는('부정')부사 "not"은 다른 단어와 붙어 있을 때 그 의미가 뚜렷해짐. n-그램 토큰화를 사용하지 않는다면 "not"은 단어 모음 안에 그냥 홀로 떨어져 있다.
    - "not"의 의미가 인접  단어가 아니라 문장 전체 또는 문서 전체와 연관될 수도 있음. 2-그램 "was not"은 단어 모음 벡터에 따로 존재하는 개별 1-그램 단어 "not" 과 "was"보다 훨씬 많은 의미를 담고 있음. 다른 말로 하면, 파이프라인에서 인접 단어들을 묶으면 각 단어의 '문맥(context)'이 어느정도 형성됨.