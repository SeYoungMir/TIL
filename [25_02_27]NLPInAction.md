# 2. 더 깊은 학습: 신경망 적용
## 6. 단어 벡터를 이용한 추론: word2vec 활용
### 2. 단어 벡터
#### 2. word2vec의 단어 표현 계산
- 스킵그램 vs CBOW : 언제 무엇을 사용
  - 미콜로프는 말뭉치가 작을 때, 그리고 자주 쓰이지 않는 단어들이 주어졌을 때 스킵그램이 잘 작동한다고 강조. 신경망의 구조상, 같은 말뭉치라도 스킵그램 접근 방식쪽이 훈련 견본을 더 많이 얻을 수 있음. 반면 연속 단어 모음 접근 방식은 자주 쓰이는 단어들에 대해 정확도가 높으며, 훈련이 훨씬 더 빠름.
- word2vec 계산 요령
- 첫 논문 발표 이후 다양한 계산 요령으로 word2vec 모형의 성능이 개선. 세 가지 요령을 탐색
- 자주 나오는 바이그램 활용
  - 함께 자주 등장하는 단어 조합이 있음. 예를 들어 "Elvis" 다음에는 "Presley"가 올 때가 많으며 이 둘은 하나의 바이그램(2-그램)을 형성. "Elvis" 다음에 "Presley"가 올 확률이 높다는 것은 그러한 예측으로부터 얻는 이득이 크지 않다는 뜻. word2vec 내장의 정확도를 높이기 위해 미콜로프는 몇몇 2-그램과 3-그램을 word2vec 어휘에 포함. 연구팀은 하나의 용어로 간주할 2- 그램들과 3-그램들을 공동 출현
  (co-occurence;또는 공기)횟수에 기초한 '점수'를 이용해 식별. 그 점수 공식은 다음과 같음.
  - $score(w_i,w_j) = {{count(w_i,w_j)-\delta}\over{count(w_i)\times count(w_j)}}$
  - 단어 $w_i$와 $w_j$의 바이그램 점수가 특정 문턱값$\delta$보다 높으면 그 둘의 쌍이 word2vec 어휘에 하나의 용어로 등록, 이 때 흔히 두 단어를 "_"같은 기호로 연결해서 하나의 용어를 만듦.
  - 실제로 word2vec 모형의 어휘를 보면 "New_York"나 "San_Fransico"같은 용어를 볼 수 있으며, 개별적인 단어의 두 원핫 벡터가 아니라 하나의 원핫 벡터로 표현
  - 이처럼 여러 단어를 조합해서 생기는 또 다른 효과는, 단어 조합의 의미가 개별 단어의 의미와 다르다는 점을 반열할 수 있다는 것. 예를 들어 MLS 프로축구 팀 이름인 Portland Timbers는 이를 이루는 개별 단어와 뜻이 다름. 따라서 이를 하나의 바이그램으로 만들어서 어휘에 추가 시 신경망 훈련 시 이 바이그램이 원핫 벡터의 개별적인 성분을 차지, 결과적으로 이 바이그램에 대한 개별적 단어 벡터가 생성.