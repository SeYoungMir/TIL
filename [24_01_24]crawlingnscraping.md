# 2. 크롤러 설계
## 2. 크롤러가 가지는 각각의 처리를 설계할 때 주의 사항
### 4. 스크레이핑과 정규 표현식
1. URL 정규화
   - 링크를 추출할 때 링크가 상대 경로인 경우가 있으므로 상대 경로를 그대로 요청에 사용하면 네트워크 요청 라이브러리가 부적절한 URL로 간주, 오류로 취급
   - 따라서 프로토콜, 호스트 이름을 포함한 절대 링크로 변환해야함
   - 현재 URL 디렉터리를 앞에 붙여주는 방식 사용
2. 테스트
   - CSS 선택자와 XPath 등을 사용할 수있는 스크레이핑 라이브러리를 사용하거나 정규 표현식을 사용하더라도 한 번에 원하는 데이터를 추출하는 경우는 흔치 않음
   - 일반적으로 여러 실패를 반복, 추출 처리 수정
   - 수정할 때마다 눈으로 결과를 확인하는 것이 굉장히 힘들기 때문에 테스트 코드를 사용 
   - 테스트 코드를 사용하면 수집 처리와 스크레이핑 처리를 분리하기 쉽고, 눈으로 결과를 하나하나 확인하지 않아도 됨
   - 입력 페이지의 HTML, XML, JSON 에서 원하는 결과가 예를 들어 숫자라면 아래와 같이 해당 함수가 제대로 동작하는지 확인하는 프로그램 작성 가능
   ```python
    def 스크레이핑_함수(입력_HTML):
        return 출력 결과 예시
    ```
   - 스크레이핑 라이브러리의 버전 업, 혹은 라이브러리를 변경했을 때 사용하면 사람이 직접 확인하면서 발생 가능한 실수를 줄일 수 있음
### 5. 데이터 저장소의 구조와 선택
- 데이터 저장소에는 다음과 같은 종류
  - 파일
  - 문서 데이터베이스(Document Database)
  - 관계형 데이터베이스(Relational Database)
  - 객체 데이터베이스(Object Database)
  - 키-값 데이터베이스(Key-Value Database)
- 규모가 작을 때와 목적에 따라서는 파일로도 충분
1. 파일
   - 가장 기본적인 데이터 저장소
   - 크롤링할 URL을 줄 바꿈으로 구분된 텍스트 파일로 저장, 혹은 내려받은 데이터들을 특정 디렉터리에 저장
   - 내려받은 내용을 보고 싶을 때는 형식에 맞는 애플리케이션으로 열기만 하면 되므로 매우 편리
   - 파일의 수가많아지면 시스템적으로 제대로 처리하기 힘들 수 있으므로 디텍터리를 나눠서 저장 또는 파일 이름으로 해시를 사용
   - 내려받은 파일이 어ㄸ너 페이지의 파일인지 부가적인 정보가 불필요, 파일의 수가 너무 많으면 파일 이름을 SHA-1 형식(Secure Hash Algorithm) 해싱하고, 앞의 두 문자로 서브 디렉터리를 만드는 등의 방법 가능
   - 디렉터리 하나에 수만 개의 파일이 들어가면 파일 목록을 추출하는 처리만으로도 문제 발생 가능
   - 디렉터리를 잘 분산할 것