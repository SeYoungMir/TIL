# 1.NLP 기초.
## 4. 단어 빈도에서 의미 찾기: 의미 분석
### 2. 잠재 의미 분석(LSA)
#### 3. 특잇값 분해
##### 5. 주제 절단
- 단어 빈도 벡터들을 ㅈ제 가중치 벡터들로 변환하는 하나의 주제 모형 생성. 현재의 주제 모형은 주제의 수가 단어의 수와 같음. 즉, 이 모형의 벡터 공간은 원래의 BOW 벡터 공간과 같은 크기. 지금까지 우리는 단어들을 다양한 비율로 혼합한 수치들을 만들어내서 그것들을'주제'라고 부르기로 한 것일 뿐. 벡터 공간의 차원은 전혀 축소되지 않음.
- LSA에서는 행렬 $S$를 무시해도 됨. 행렬 $U$에 이미 중요한 주제에 해당하는 성분들(큰 특잇값들)이 왼쪽으로 배치되어 있기 때문. $S$를 무시해도 되는 또 다른 이유는 이 모형과 함께 사용하고자 하는 단어-문서 벡터들(TF-IDF 벡터들)이 대부분 정규화되어있기 때문. 실제로 많은 경우 행렬 $S$의 모든 대각 성분을 1로 설정하면 오히려 더 나은 주제 모형이 생성.
- 실제로 $U$의 오른쪽에서 열들을 제거해서 주제의 수를 축소. 그러려면 문서의 본질을 포착하기에 충분한 주제의 수를 가늠할 필요가 있음. LSA의 정확도를 측정하는 한 가지 방법은 주제-문서 행렬로부터 용어-문서 행렬을 얼마나 정확하게 재구축(복원)할 수 있는지를 보는 것. 다음 코드는 $U$의 열들을 점차 제거, 어휘 단어 수가 9이고 행렬의 문서 수가 11인 용어-문서 행렬을 재구축, 그 오차를 계산하는 코드.
- ```python
  err = []
  for nnumdim in range(len(s),0,-1):
        S[numdim -1, numdim -1]=0
        reconstructed_tdm= U.dot(S).dot(Vt)
        err.append(np.sqrt(((reconstructed_tdm-tdm).values.flatten()**2.sum()/np.product(tdm.shape))))
  np.arrat(err).round(2)   
  ```
- 마지막 출력에서 보듯, U의 열벡터들을 제거(S의 해당 성분을 0으로 설정해서)할수록 재구축 오차가 커짐 이전 사고 실험처럼 주제를 세 가지로 한정해서 재구축한 BOW 벡터들은 원래의 벡터들과 28%만큼 다름 다음 그림은 주제 모형의 차원 축소에 따른 정확도 감소 추세 그래프.
- 그래프에서 보듯 TF-IDF 벡터를 사용하든 BOW 벡터를 사용하든 재구축 정확도는 비슷한 기울기로 감소. 그래도 TF-IDF 벡터들이 약간 더 나으며, 제거한 주제가 많을수록 BOW와의 차이가 커짐.
- ![alt text](image-6.png)
- 간단한 예여도 주제(차원)의 수를 몇 개로 유지하는 것이 좋은지를 그래프를 이용해 파악하는 방법을 알 수 있음. 때에 따라서는 용어0문서 행렬에서 상당히 많은 수의 차원을 제거해도 정확도가 꽤 높게 나오기도 함.
- LSA에 쓰이는 SVD 알고리즘은 항상 함께 등장하는 단어들을 '인식', 하나의 주제로 결합.
- 이것이 LSA가 다수의 차원을 쉽게 압축하는 방법. NLP 프로젝트가 주제 모형을 사용하지 않는다고 해도, 단어 -문서 행렬을 압축하거나 파이프라인이 처리할 잠재적인 복합어 또는 n-그램들을 식별하고 싶을 때 LSA(SVD)가 유용.