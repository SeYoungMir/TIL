# 1.NLP 기초.
## 1. 사고의 단위: NLP 개요
### 4. 컴퓨터 입장으로 언어 읽기 
#### 4. 다른 접근 방식 - 이어서
- ```python
  from collections import Counter
  Counter("Guten Morgen Rosa".split())
  Counter("Good morning, Rosa!".split())
  ```
- 이런 토큰들을 좀 더 깔끔하게 정리하는 방법은 다음장에서 소개.
- 이런 고차원 희소 벡터(모든 가능한 단어마다 하나의 '통'이 있지만 대부분의 통이 비어 있는)가 자연어 처리에 아주 유용하지는 않을 것이라고 짐작할 수 있으나, 이런 도구도 스팸 필터 같은 몇몇 '업계를 뒤바꾼' 도구들에는 충분히 유용, 이에 관해서는 이후에 논의
- 하나의 문장을 분류기에 공급하면, 분류기의 바닥에 있는 각 통에는 그 문장에 등장하는 각 토큰들이 쌓임. 각 통의 토큰 개수로 이루어진 벡터를 해당 문서의 벡터 표현이라고 부르고, 그런 식으로 모든 가능한 문서, 문장, 심지어 개별 단어로부터 얻을 수 있는 모든 가능한 벡터들의 집합을 벡터 공간이라고 부름. 그리고 문서, 문장, 단어의 이러한 모형을 벡터 공간 모형(vector space model)이라고 부름
- 선형 대수학(linear algebra)의 수단들로 이런 벡터들을 조작함으로써 자연어 문장들의 통계량이나 거리를 계산할 수 있으며, 그런 수치들을 이용하면 인간 프로그래머의 개입 필요성과 NLP 파이프라인의 허약함을 줄이고 좀 더 다양한 자연어 처리 문제를 풀 수 있음.
- 단어 모음 벡터 표현에 관한 통계적 질문 중 하나는 "특정 단어 모음 다음에 등장할 가능성이 가장 큰 단어 조합은 무엇인가" 고, 실용적인 예를 들면 사용자가 입력한 일련의 단어들에 해당하는 단어 모음 벡터에 가장 가까운 단어 모음을 응용 프로그램의 단어 모음 데이터베이스에서 검색하는 문제로, 가장 기본적인 웹 검색 엔진의 예에 해당
- 검색 엔진은 사용자가 검색창에 입력한 단어들로 이루어진 단어 모음에 가장 가까운 단어 모음 벡터가 있는 웹 문서를 검색, 이 두 문제를 효율적으로 해결하는 수단이 있다면, 사용자와 대화를 거듭하여 자료를 수집할 수록 점점 더 나아지는 기계 학습 챗봇을 만드는 것이 가능.
- 이런 희소 고차원 벡터를 처음 접하는 이들도 있음. 단어 모음 벡터는 차원(성분 개수)이 매우 높음. 커다란 말뭉치에서 추출한 3-그램 어휘를 위한 벡터는 그 차원이 수백만 정도일 수 있음. 다음 장에서는 차원의 저주를 비롯해 고차원 벡터를 다루기 어렵게 만드는 몇 가지 성질을 논의