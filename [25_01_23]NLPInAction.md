# 2. 더 깊은 학습: 신경망 적용
## 5. 신경망 첫걸음: 퍼셉트론과 역전파
### 1. 신경망의 구성 요소
#### 3. 치우침 단위
##### 다음 단계
- 새로운 신경망이 복잡합(비선형)문제를 풀긴 했지만, 당시로써는 계산량이 과도함. 논리 회로 소자 하나나 코드 한 줄로 풀 수 있는 XOR 문제를 푸는데 두 개의 퍼셉트론과 상당한 분량의 역전파 계산이 필요하다는 것은 컴퓨터 계산 능력의 낭비로 간주. 일상적인 용도로 사용할 수 없음이 판명, 신경망은 다시 학계와 슈퍼컴퓨터 실험의 세계로 물러남.
- 약 1990년~2010년 사이의 인공지능의 두 번째 암흑기를 지난 후, 시간이 지나 컴퓨터의 계싼 능력이 급격히 증가, 사용 가능한 원본 자료가 넘쳐나게 되고 역전파 알고리즘이 개선됨. 알고리즘의 계산 비용과 자료 집합의 제한이 더 이상 신경망의 발목을 잡지 않게 되면서 2010년대 초중반에 신경망이 세 번째 부활.
- 루멜하트와 매클리랜드에 의한 두 번째 부흥기의 성과를 살펴봄.
##### 두 번째 ㅂ흥기
- 대부분의 위대한 착상이 그렇듯, 좋은 착안은 언젠가는 수면 밖으로 떠오름. 루멜하트 등은 애초에 퍼셉트론의 몰락을 부른 본질적 결함을 극복하는 방법을 발견. 
- 착안의 핵심은 하나의 퍼셉트론으로 예측을 수행하는 대신, 여러 개의 퍼셉트론을 함께 사용한다는 것. 하나(또는 여러 개)의 퍼셉트론에 입력을 넣고, 그 퍼셉트론의 출력을 다른 퍼셉트론의 입력에 연결, 하나의 '네트워크(망)'을 형성. 그 네트워크의 끝에 있는 퍼셉트론의 출력이 전체 신경망의 예측 결과. 이러한 퍼셉트론의 네트워크, 즉 다층 신경망은 좀 더 복잡한 패턴들을 학습 가능, XOR 문제에서처럼 서로 다른 부류를 선형으로 분리할 수 없다는 문제점을 극복. 이러한 착안을 실현하는 데 있어 핵심은 출력층 이전의 층들에 있는 가중치들을 어떻게 갱신할지.
- 가중치 갱신 과정의 주요 부분을 공식화. 예측 결과의 오차에 기초해서 퍼셉트론의 가중치를 갱신하는 방법을 학습. 그러한 오차를 측정하는 함수를 비용함수(cost function) 또는 손실함수(loss function)이라고 함.
- 비용함수는 신경망에 입력된 '질문', 즉 입력 견본 $x$에 대해 신경망이 출력해야하는 정답과 싱견망이 실제로 출력한 예측값 $y$의 차이를 수량화하는 함수.
- 다음 비용 함수의 한 예는 참값(정답)과 모형의 예측값의 차이의 절댓값을 오차 또는 비용으로 산출
  - $err(x)=|y-f(x)|$
- 퍼셉트론 훈련의(그리고 일반적으로 신경망 훈련의)목표는 모든 가능한 입력 표본에 대해 이 비용함수를 최소화 하는 것. 다음 식은 이러한 최적화 문제를 수식으로 표현
  - $J(x)= min\Sigma^n_{i=1}err(x_i)$
- 이후에 평균제곱오차 같은 다른 비용함수들도 알게 되어도 최상의 비용함수가 무엇인지 선택해야 할 일은 없을 것. 대부분의 신경망 프레임워크에는 최상의 비용함수가 이미 결정되어있기 때문.
- 논의에서 기억할 것은 자료 집합 전체에 대해 비용함수를 최소화하는 것이 신경망 훈련의 궁극적인 목표라는 점. 이 점을 기억하고 있으면 앞으로 이야기할 나머지 개념들이 자연스럽게 이해 됨.