# 1.NLP 기초.
## 2. 단어 토큰화 - 이어서
- 생략된 단어나 뭔가를 암시하는 단어도 고민할 필요가 있음. 한 단어로 된 명령문 "Don't!"와 같은 경우 어떤 단어가 생략되어 있는지 생각해보면 최소한 세 개의 단어가 생략되어 있을 수 있다. 보통 you, do, that과 같은 단어가 생략되어 있을 테이며, 컴퓨터는 원래의 다섯 토큰 중 생략된 세 개의 의미 전달 단위를 파악할 수 있어애 함. 하지만 지금은 생략어를 고려하지 말고, 토큰들을 주어진 대로 식별하는 토큰 생성기를 구축. 생략이나 암시, 문장의 의미 자체는 다음에 다룸
- 하나의 문자열을 단어들로 분할하는 단순한 알고리즘들을 소개. 
- 토큰 두 개, 세 개, 네 개, 심지어 다섯 개로 이루어진 쌍(pair)들을 추출하는 방법도 살펴봄. 
- 이런 토큰 쌍을 n-그램이라고 부르는데, n은 토큰(단어)개수.
- 예를 들어 단어 두 개의 쌍을 2-그램(바이그램,bigram),세 개는 3-그램(트라이그램, trigram), 네 개는 4- 그램등임.
- n-그램을 이용하면 컴퓨터는 "ice"와 "cream"뿐만 아니라 그 둘로 이루어진 "ice cream"도 인식할 수 있음. 챗봇이 인식하면 좋을 만한 또 다른 2-그램으로는 "Mr. Smith"와 같은 경우가 있는데, 문서의 토큰 모음과 벡터 표현에는 "Mr."와 "Smith"뿐만 아니라, "Mr. Smith"도 한 자리를 차지하게 됨.
- 지금은 모든 가능한 두 단어 쌍(그리고 그리 길지 않은 n-그램들)을 어휘에 포함하기로 함. 다음 장에서는 단어의 중요도를 단어가 문서에 출현한 횟수에 기초해서 추정하는 방법을 학습. 그러면 인접해서 출현하는 경우가 거의 없는 2-그램과 3-그램들을 제외할 수 있음. 여기서 제시하는 접근 방식들이 완벽한 것은 아님. 기계 학습 파이프라인의 특징 추출기가 입력 자료에 담긴 모든 정보를 유지하는 경우는 거의 없음. 특정 응용을 위해 텍스트에서 더 많은 또는 이전과는 다른 정보를 추출하도록 토큰 생성기를 수정해야 할 때가 언제인지를 아는 것은 NLP 개발자의 중요한 능력 중 하나.
- 자연어 처리에서 텍스트를 수치 벡터로 변환하는 것은 특히나 "손실이 큰"특징 추출 과정. 그렇긴 하지만 단어 모음 벡터는 텍스트에 담긴 정보를 유용하고 흥미로운 기계 학습 모형을 산출하기에 충분할 정도로 유지.
- 감정 분석기를 위한 기법들은 Gmail이 스팸 홍수(거의 모든 이메일을 쓸모없게 만드는)로부터 사용자를 보호하는 데 사용하는 것과 같은 기법임.