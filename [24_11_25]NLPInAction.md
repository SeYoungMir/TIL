# 1.NLP 기초.
## 4. 단어 빈도에서 의미 찾기: 의미 분석
### 2. 잠재 의미 분석(LSA)
#### 3. 특잇값 분해
- SVD를 BOW 용어 - 문서 행렬에 적용하든 아니면 TF-IDF 용어 - 문서 행렬에 적용하든, SVD는 같은 주제에 속하는 단어들의 조합을 탐색. SVD는 용어 - 문서 행렬의 열들(용어들) 사이의 상관계수를 계산함으로써 그런 공동 출현 단어들을 탐색.
- 그와 동시에 SVD는 문서들 사이의 용어 빈도 상관 관계와 문서들 사이의 상관관계도 파악. 이러한 두 종류의 정보에 기초, SVD는 말뭉치 전체에서 분산이 가장 큰 용어들의 일차결합들을 계산.
- 이러한 용어 빈도들의 일차결합이 곧 주제. 말뭉치에서 분산이 큰(따라서 많은 정보를 담은) 주제들만 남기고 다른 주제들을 제거함으로써 말뭉치 표현의 벡터 공간을 축소. 또한, SVD는 용어 - 문서 벡터들을 적절히 회전, 각 문서에 대한 주제 벡터를 더 짧게 만드는 방법도 표시.
- SVD는 상관관계가 큰(즉, 같은 문서에 나오는 경우가 많은), 그러면서 문서들 전체적으로 빈도의 차이가 큰 단어들을 하나로 결속. LSA는 그런 단어들의 일차결합을 '주제'로 간주.
- 즉, SVD는 BOW 벡터들(또는 TF-IDF 벡터들)을 문서가 말하는 주제들을 반영한 주제 벡터들로 변환, 하나의 주제 벡터는 문서가 말하는 바를 해당 BOW 벡터보다 훨씬 낮은 차원의 벡터로 요약 또는 일반화한 것이라 할 수 있음.
- SVD를 단어 출현 횟수에 적용해서 주제 벡터를 만들어 낸다는 생각을 처음 떠올린 사람이 누구인지는 명확하지 않음. 여러 언어학자가 비슷한 접근 방식을 동시에 연구, 그들은 모두 두 자연어 표현(또는 개별 단어들)의 의미상의 유사도가 단어들 또는 문구들이 쓰인 문맥들 사이의 유사도에 비례한다는 점을 발견. 그런 결과를 담은 연구 문헌이 다양.
- 다음은 LSA를 위한 SVD의 정의
  - $W_{m\times n}\rArr U_{m\times p}S_{p\times p}{V_{p \times n}}^T$
- 위 공식에서 m은 어휘의 단어 수, n은 말뭉치의 문서 수, p는 말뭉치의 주제 수. 지금 단계에서 주제 수 p는 단어 m과 같음. 즉, 아직은 SVD가 문서 표현 벡터 공간의 차원을 축소하지 않음. 이후 과정에서 불필요한 주제들이 제거, TF-IDF벡터보다 낮은 차원의 주제 벡터들을 생성, 비로소 차원이 축소. 지금 단계에서는 모든 차원이 그대로 유지.
- 분해된 세 행렬($U,S,V$)를 살펴봄.