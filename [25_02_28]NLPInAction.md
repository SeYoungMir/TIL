# 2. 더 깊은 학습: 신경망 적용
## 6. 단어 벡터를 이용한 추론: word2vec 활용
### 2. 단어 벡터
#### 2. word2vec의 단어 표현 계산
- 빈도 높은 토큰의 부표집
  - 원래의 word2vec 알고리즘을 개선하는 또 다른 기법으로, 빈도 높은(자주 출현하는) 단어의 부표집(subsamlpling;또는 이단 추출)이 있음. "the"나 "a"같이 흔히 쓰이는 단어들은 문장에서 그리 중요한 의미를 지니지 않을 때가 많음. 그리고 "the"는 말뭉치에서 아주 다양한 단어들과 함께 출현하므로, "the" 자체는 단어들 사이의 관계에 관해 그리 많은것을 말해주지 않음. 오히려 의미 유사도 기반 훈련에 기초한 word2vec 표현의 정확도에 해가 됨.
    - 모든 단어는 의미를 지니며, 불용어들도 예외는 아님. 따라서 단어 벡터들을 훈련하거나 어휘를 구축할 때 불용어들을 완전히 무시하거나 배제하는 것은 바람직하지 않음. 또한 단어 벡터는 생성적 모형에도 흔히 쓰이는데, 그런 경우 불용어나 기타 흔한 단어들을 어휘에 포함하고, 그것들이 이웃 단어들의 단어 벡터에 영향을 미칠 수 있게 해야함
  - 훈련 과정에서 단어들을 그 빈도에 반비례해서 표집 시 불용어 같은 빈도 높은 단어들이 단어 벡터들에 미치는 영향을 줄일 수 있음. 이러한 '부표집'기법의 효과는 IDF(역문서빈도)가 TF-IDF 벡터에 미치는 효과와 유사. 즉, 자주 출현하는 단어들이 드물게 나오는 단어들보다 벡터에 더 적은 영향을 미침. 다음은 미콜로프가 주어진 단어의 표집 확률을 구하는 데 사용한 공식. 이 확률은 훈련 과정에서 주어진 단어가 주어진 스킵그램에 포함되는지 여부를 결정
    - $P(w_i)=1-\sqrt{t\over{f(w_i)}}$
  - word2vec C++ 구현은 논문에 나온것과는 조금 다른 표집 확률을 사용, 그 효과는 동일.
    - $P(w_i)={{f(w_i)-t}\over{f(w_i)}}-\sqrt{t\over{f(w_i)}}$
  - 위 공식들에서 $f(w_i)$는 단어 $w_i$의 출현 확률(빈도를 어휘 크기로 나눈 것)이고 $t$는 일종의 문턱값. 기본적으로 출현 확률이 이 문턱값보다 큰 단어에 대해 부표집이 적용. 이 문턱값은 말뭉치 크기와 평균 문서 길이, 그리고 문서들에 쓰인 단어들의 다양성에 기초 , 적절히 설정해야 함. 연구 문헌들에는 $10^{-5}$ 에서 $10^{-6}$사이의 값이 흔히 
  쓰임.
  - 말뭉치에 쓰인 서로 다른 단어가 100만개이고 부표집 문턱값이 $10^{-6}$이라고 할 때, 말뭉치에 10번 출현한 단어가 토큰화 과정에서 임의의 n-그램에 포함될 확률은 68%. 다른 말로는 32%의 경우에서 무시됨.
  - 미콜로프는 이러한 부표집 기법이 비유 질문에 답하는 등의 과제에서 단어 벡터의 정확도를 개선함을 증명.