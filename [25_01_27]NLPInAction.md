# 2. 더 깊은 학습: 신경망 적용
## 5. 신경망 첫걸음: 퍼셉트론과 역전파
### 1. 신경망의 구성 요소
#### 3. 치우침 단위
##### 모든것을 미분.
- 활성화 함수가 미분가능 함수라고 할 때, 출력층에 있는 뉴런의 가중치는 아주 간단히 갱신 가능. 출력층의 $j$번째 뉴런의 가중치 변화향은 다음 식처럼 $j$ 번째 출력에 대한 오차의 미분(편미분)으로 주어짐
  - $\Delta w_{ij} = -\alpha {{\delta E}\over{\delta w_{ij}}} = \alpha y_{i}(y_j-f(x)_j)y_i(1-y_i)$
- 은닉층의 가중치 변화량도 이와 기본적으로 같으나 편미분의 전개가 좀 더 복잡함
  - $\Delta w_{ij} = -\alpha {{\delta E}\over{\delta w_{ij}}} = \alpha y_{i}(\Sigma_{l\in L}\delta_{l}w_{jl})y_i(1-y_i)$
- 위의 두 식에서 f(x)는 출력 벡터이고 아래 첨자 $j$는 그 벡터의 $j$번째 성분을 뜻함. $y$는 그 아래 첨자에 따라 $i$ 번째 층 또는 $j$번째 층에 있는 한 노드의 출력. $i$번째 층의 출력이 $j$번째의 입력이라는 점을 기억. 이 공식들은 $i$번째 층과 $j$번째 층을 연결하는 가중치의 변화량이 그 가중치에 대한 오차의 편미분에 학습 속도 $\alpha$를 곱한 것임을 말해 줌. 
- 아래 식은 중간층(은닉층)에 관한 것이기 때문에 그 이전 층에서 온 모든 입력의 합이 편미분에 관여(시그마 부분)
- 이러한 공식들로 가중치들을 '언제' 갱신할 것인가도 중요한 문제. 각 층에서 각 가중치를 갱신할 때, 관련된 모든 계산은 순전파 단계에서 산출된 네트워크의 상태에 의존. 순전파의 끝에서 신경망의 전체 오차를 계산 한 후 다시 거꾸로 가면서(역전파) 각 가중치의 변화량을 계산하되, 그것으로 가중치를 실제로 갱신하면 안됨. 그런 식으로 신경망의 시작부분까지 도달한 후에야 실제로 갱신해야 함. 그렇게 하지 않고 그때 그때 가중치를 갱신 시 신경망 앞쪽(입력 족)으로 갈수록 미분들이 부정확해져서 가중치 변화량도 틀리게 나옴.
- 이상의 공식들에 기초한 훈련 과정을 간략히 설명하면 다음과 같음. 신경망에 훈련 자료의 한 견본을 입력 > 그 입력이 순전파를 통해 신경망의 출력층에 도달, 출력갑 출력 > 오차 계산, 그 오차를 출력층에서 입력층 쪽으로 역전파하면서 각 층의 각 가중치들을 앞의 공식들을 이용해서 갱신. 이 과정을 훈련 자료의 모든 견본에 대해 반복. 모든 입력에 대해 가중치들을 갱신하는 훈련 주기를 하나의 세(epoch; 세대, 에폭)또는 세대라고 부름. 이러한 훈련 주기(세)들을 더 반복해서 가중치들을 더욱 정련할 수도 있지만, 훈련 집합에 과대적합할 수 있음에 주의. 과대적합 발생 시 훈련 집합에 없는 새로운 자료점에 대해 신경망이 의미 있는 예측 결과를 산출하지 못함.
- 두 식의 $\alpha$는 학습 속도(learning rate)임. 즉 , 이 초매개변수는 주어진 훈련 주기 또는 자료 배치에서 가중치를 관측된 오차에 기초해서 얼마나 보정할 것인지를 결정. 일반적으로 한 훈련 주기에서 학습 속도를 상수로 두지만, 몇몇 더 정교한 훈련 알고리즘들은 학습 속도를 높이고 수렴을 보장하기 위해 학습 속도를 동적으로 변경. $\alpha$가 너무 크면 가중치가 너무 크게 변해서 최적해를 건너뛰고, 그 다음 갱신에서는 가중치가 그 반대방향으로 이전보다도 더 크게 변해서 최적해에서 더욱 멀어지는 현상이 발생 가능. 반대로 $\alpha$가 너무 작으면 수렴에 도달하는 시간이 비현실적으로 길어질 수 있음. 그보다 나쁜 일은 오차 곡면의 극소점을 벗어나지 못하는 것.