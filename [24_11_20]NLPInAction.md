# 1.NLP 기초.
## 4. 단어 빈도에서 의미 찾기: 의미 분석
### 2. 잠재 의미 분석(LSA)
- 잠재 의미 분석은 임의의 벡터들에 대한 선형 변환(회전과 비례)을 수행하는 '최선의; 방법을 찾기 위한 수학 기법이라 할 수 있음. NLP의 경우 선형 변환 대상은 TF-IDF 벡터나 BOW 벡터. 
- 여러 NLP 벡터에서 그러한 '최선의' 방법은 단어 빈도들의 분산(variance)이 최대가 되도록, 다시 말해 단어 빈도들의 차이가 최다한 커지도록 새 벡터(변환된 벡터)들의 축(차원)을 정렬하는 것. 그런 식으로 새 벡터 공간을 구했다면, 여러 문서에 대한 벡터들의 분산에 별로 기여하지 않는 축들을 제거함으로써 벡터 공간을 축소 가능.
- 이런 방식의 특잇값 분해를 절단된 특잇값 분해(truncated singular value decomposition)이라 하고, 줄여서 절단된 SVD라고 부름. 이 기법은 이미지 처리와 이미지 압축 분야에서 사용하는 주성분 분석(principal component analysis, PCA)와 동일. 
- 이번 장에서 LSA 벡터의 정확도를 개선하는 몇 가지 요령을 소개, 그 요령들은 다른 분야의 기계 학습과 특징 공학(feature engineering) 문제를 PCA로 푸는데도 도움이 됨.
- 선형 대수에 익숙한 사람이라면 SVD에 깔린 수학 이론을 알고 있고. 이미지나 기타 고차원 자료에 대한 기계 학습을 수행해 본 사람은 그런 고차원 벡터에 PCA를 적용하는 방법을 알고 있을 것. 자연어 문서에 대한 LSA는 TF-IDF 벡터들에 대해 PCA를 적용하는 것에 해당.
- LSA는 SVD를 이용해서 자료의 가장 큰 분산의 원인이 되는 단어 조합을 탐색. TF-IDF 벡터들을 이리저리 회전, 단어 빈도들의 분산이 가장 커지게 함. SVD는 바로 그러한 최대 분산 방향들을 수학적으로 탐색.
- '편향 벡터 (bias vector)' 라고 부르는 방향들을 좌표축들로 하는 새 벡터 공간은 세 개의 6차원 주제 벡터들로 형성된 벡터 공간과 동일. 이 경우 각 축은 하나의 단어 빈도가 아니라 단어 빈도들의 조합. 즉 각 축은 말뭉치 전체에 쓰이는 다양한 '주제'들을 형성하는 단어들의 가중 결합.