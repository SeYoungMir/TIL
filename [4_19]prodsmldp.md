## 분류:KNN(K-nearest Neighbor Classifier)
### Keywords
- 유클리디안 거리
- 맨해튼 거리
- 민코우스키 거리
### KNN(K-nearest Neighbor)
- KNN 알고리즘
  - 가장 간단한 지도학습 머신러닝 알고리즘
  - 훈련 데이터를 저장해 두는 것이 모델을 만드는 과정의 전부임
  - K개의 가장 가까운 이웃 중에서 어떤 범주가 가장 비중이 높은가?
  - 새로운 데이터가 입력되면 그 새로운 데이터 주변의 가장 가까운 K개의 훈련 데이터의 레이블을 확인한 뒤, 가장 많이 보이는 라벨로 분류하는 방법.
- K의 결정
  - KNN에서의 K의 결정은 매우 중요한 문제임.
  - K가 작으면 이상점 등의 노이즈에 민감하게 반응하는 과적합의 문제
  - K가 크면 자료의 패턴을 잘 하악할 수 없어 예측 성능이 저하됨
  - 검증용(validation)데이터를 이용하여 주어진 훈련 데이터에 가장 적절한 K를 찾아야 함.
- 거리의 측정
  - $n$개의 특성변수를 가지는 자료에서 두 개의 관찰점$a=(a_1,a_2,...,a_n)$와$b=(b_1,b_2,...,b_n)$간의 거리를 측정하는 문제.
  - 유클리디안 거리<br>$d(a,b)=\sqrt{(a_1-b_1)^2+(a_2-b_2)^2+\cdots+(a_n-b_n)^2}$
  - 맨해튼 거리<br>$d(a,b)=|a_1-b_1|+|a_2-b_2|+\cdots+|a_n-b_n|$
  - 민코우스키 거리<br>$d(a,b)=({\Sigma_{i=1}^n{|a_i-b_i|^p}})^{1\over p}$
  - 자료에 스케일에 차이가 있는 경우, 스케일이 큰 특성변수에 의해 거리가 결정되어버릴 수 있음. 따라서 각 특성변수 별로 스케일이 유사해지도록 표준화 변환(Z score)또는 min-max변환으로 스케일링을 해준 뒤 거리를 재는 것이 적절함.
