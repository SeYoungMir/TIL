# 1.NLP 기초.
## 3. TF-IDF 벡터
### 4. 주제 모형화
#### 5. Okapi BM 25
- 런던 시티 대학교(City University London)의 사람들이 검색 결과의 순위를 좀 더 잘 매기는 방법을 고안. 그들의 방법은 TF-IDF 코사인 유사도를 정규화, 평활화한 값을 사용
- 질의 문서에서 중복된 단어들을 제거, 질의 문서 벡터의 용어 빈도가 1을 넘지 않게 함. 코사인 유사도를 정규화할 때는 TF-IDF 벡터 노름(말뭉치 문서나 질의문에 있는 용어 개수)이 아니라 문서 길이 자체의 비선형 함수를 분모로 사용. 이 방법에 쓰이는 코사인 유사도는 파이썬 코드로 다음과 같음
- ```python
  q_idf * dot(q_tf,d_tf[i])* 1.5/(dot(q_tf,d_tf[i]+.25+.75*d_num_words[i]/d_num_words.mean()))
  ```
- 사용자가 원하는 것에 좀 더 가까운 검색 결과를 제시하고자 할 때는 이처럼 유사도의 정규화나 가중치 적용 방식을 조율해 보는 것이 도움이 됨. 그러나 말뭉치가 그리 크지 않다면 좀 더 근본적인 변화 필요, 특히 단어의 의미를 좀 더 정확하게 반영할 수 있는 문서표현 방식을 도입할 필요 있음.
- 이후에는 단순히 질의문에 있는 단어들이 많이 나온 문서들을 찾는 것이 아니라, 질의문의 단어들에 담긴 '의믜'를 담고 있을 가능성이 큰 문서들을 찾는 방식의 의미론적 검색 엔진을 구현.
- TF-IDF의 가중치 적용 방식이나 정규화 방식, 어간 추출, 표제어 추출을 아무리 조율해도 이런 의미론적 검색보다 우월한 성과를 얻기는 힘듦. 구글이나 빙(Bing)같은 웹 검색 엔진들이 의미론적 검색을 사용하지 않는 이유는 말뭉치가 너무 크기 때문.
- 문서 개수가 수십억 규모인 말뭉치에서는 의미론적 단어 벡터나 주제 벡터가 비효율적, 그러나 문서 수백만건 정도는 의미론적 검색도 감당 가능.
- 정리하자면, 의미론적 검색이나 문서 분류, ㄷ화 시스템, 그리고 앞에서 언급한 대부분의 NLP 응용에서는 그냥 가장 기본적인 TF-IDF 벡터들을 파이프라인에 공급하는 것으로도 충분
- 파이프라인의 첫 단계에서 TF-IDF벡터들은 NLP를 위해 텍스트에서 추출하는 가장 기본적인 특징 집합에 해당. 다음 장에서는 이 TF-IDF 벡터들에서 주제 벡터들을 계산하는 방법을 탐색.
- TF-IDF벡터를 아무리 세심하게 정규화하고 평활화해도 주제 벡터보다 단어 모음의 의미를 잘 표현하지는 못함. 더 나아가서 word2vec 벡터와 그 이후 장들에서 논의하는 단어 및 문서 의미의 신경망 내장은 문서의 의미를 주제 벡터보다 더 잘 표현.
#### 6. 다음 단계
- 자연어 텍스트를 수치들로 변환할 수 있으면, 여러 수학 도구들을 이용해서 그 수치들을 다양한 방식으로 계산하고 조작. 이번 장에서 구한 단어 통계 수치에 기초해서, 다음 장에서는 개별 단어에 관한 수치가 아닌 그 단어들로 이루어진 문서의 의미 또는 주제를 수치로 표현하는 방법을 탐색.