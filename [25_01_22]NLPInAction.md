# 2. 더 깊은 학습: 신경망 적용
## 5. 신경망 첫걸음: 퍼셉트론과 역전파
### 1. 신경망의 구성 요소
#### 3. 치우침 단위
##### 다음 단계
- 위의 기본적인 퍼셉트론에는 본질적인 결함이 있음. 만일 자료가 선형 분리가능(linearly separable)이 아니면, 다른 말로 해서 서로 다른 부류의 자료점들을 직선(선형)으로 분리할 수 없으면 모형은 절대로 수렴하지 않음. 즉,. 모형은 최적의 가중치들을 배우지 못하며, 따라서 목표변수를 정확하게 예측할 수 없음.
- 퍼셉트론에 관한 초기 실험들은 오직 주어진 예제 이미지들과 그 부류들만으로 이미지들을 분류하는 방법을 퍼셉트론이 배울 수 있음을 보여줌. 그러나 이 성과에 대한 초기의 흥분은 민스키와 패퍼트의 비판에 희해 사그라듦. 퍼셉트론에 관한 그들의 책에서 민스키와 패퍼트는 퍼셉트론으로 수행할 수 있는 분류 작업의 종류가 극히 제한적임을 보임. 그들은 자료점들을 이산적인 그룹들로 선형 분리할 수 없으면 퍼셉트론은 입력 자료를 분류하는 방법을 배우지 못함을 증명.
- ![alt text](image-10.png)
- 자료점들이 위 그림처럼 선형 분리 가능이면 퍼셉트론은 문제없이 작동. 그러나 다음 그림처럼 선형분리 불가능 자료점들은 그렇지 않음. 이 경우는 직선을 어디에 두든 두 부류(둥근 점과 X 표시)의 자료점들을 가르지 못함. 이처럼 서로 다른 부류의 자료점들이 섞여있으면 단일 뉴런 퍼셉트론은 최적의 해에 도달하지 못하고 가중치 공간을 계속 방황. 결과적으로 그냥 동전을 던지는 것과 같은 무작위 추측보다 나은 성과를 낼 수 없음.
- ![alt text](image-11.png)
- 퍼셉트론은 자료 집합의 특징들과 목표 변수 사이의 관계를 서술하는 하나의 직선 방정식을 찾음. 즉, 퍼셉트론이 하는 일은 선형회귀(linear regression). 퍼셉트론은 비선형 방정식이나 비선형 관계를 배우지 못함.
- 자료 값들 사이의 관계 중 선형이 아닌 것들이 많이 있으며, 대체로 그런 관계들은 선형회귀 모형이나 직선 방정식으로 서술할 수 없음. 실제 응용에 필요한 자료 집합은 서로 다른 부류의 자료값들을 직선이나 평면으로 분리할 수 없는 것들이 대부분, 민스키와 패퍼트가 제시한' 증명'은 퍼셉트론의 한계를 제시. 
- 그러나 퍼셉트론에 깔린 착안은 쉽게 사라지지 않음. 민스키 등의 책이 나온 지 17년 후, 물리학자 루멜하트와 심리학자 매클리랜드의 공동연구는 퍼셉트론을 여러 개 함께 사용 시 비선형 관계를 가진 XOR(eXclusive OR:배타적 논리합)문제를 풀 수 있음을 보임. 앞에서 우리가 살펴본 모형은 다층 역전파를 요구하지 않는 단일 퍼셉트론 모형, 이런 모형으로는 논리합 문제 같은 단순한 문제들만 풀 수 있음. 루멜하트와 매클리랜드는 여러 개의 퍼셉트론을 함께 사용하고, 오차를 각 퍼셉트론에 적절한 비율로 배분함으로써 기존 퍼셉트론의 한계를 극복. 이를 위해 사용한 것이 바로 현재 기계 학습의 핵심인 역전파. 사실 역전파 알고리즘 자체는 더 오래전에 제시된 알고리즘.여러 층의 뉴런들 사이에서 오차를 역전파해서 가중치들을 갱신한다는 착안에서 최초의 현대적인 신경망이 탄생.