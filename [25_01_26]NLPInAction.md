# 2. 더 깊은 학습: 신경망 적용
## 5. 신경망 첫걸음: 퍼셉트론과 역전파
### 1. 신경망의 구성 요소
#### 3. 치우침 단위
##### 역전파
- '오차의 역전파'를 줄인 역전파 알고리즘은 신경망의 입력과 출력(예측값), 그리고 바람직한 목푯값이 주어졌을 때 특정 가중치의 적절한 변화량(갱신량)을 구하는 알고리즘. 순전파(forward propagation) 단계에서는 입력이 신경망을 따라 '앞으로',즉 순방향으로 나아가서 결국에는 신경망의 출력이 계산. 그와는 반대 방향의 역전파를 위해서는 뉴런의 활성화 함수를 약간 더 복잡한 형태로 변경.
- 이전에는 계단함수를 뉴런의 활성화 함수로 사용, 그러나 잠시후에 보갰자먼 역전파를 위해서는 비선형 연속 미분가능(continuously differentiable)함수를 활성화 함수로 사용해야 함. 이런 활성화 함수를 사용하는 뉴런은 두 값 사이의(이를테면 0과 1 사이의)연속값을 출력. 활성화 함수로 흔히 쓰이는 S자형 함수(sigmoid function)가 그런 조건을 만족하는 함수.
  - $S(x) = {{1}\over{1+e^{-x}}}$
- 이외에도 쌍곡 탄젠트(hyperbolic tangent) 함수나 정류 선형 단위(rectified linear unit, RLU)같은 활성화 함수들이 있는데 각자 장단점이 다름. 이들은 모두 ㅇ러 신경망 구조에서 다양한 쓰임새가 있으므로, 이후의 장들에서 따로 설명.
- 활성화 함수가 미분가능이어야 하는 이유는. 어떤 함수의 미분을 계산할 수 있다면, 그 함수 자체의 여러 변수에 대한 함수의 편미분들도 계산할 수 있음. 신경망의 중점은 "여러 변수에 대한 함수의 편미분들"이고, 한 가중치의 해당 뉴런이 받은 입력에 대한 변화량을 계산할 수 있는 이유가 편미분 덕분이기 때문.
##### 모든것을 미분.
- 신경망의 오차(출력과 목푯값의 차이)를 제곱한 제곱오차(squared error)를 비용 함수로 사용한다고 하자
  - $SE = (y-f(x))^2$
- 그런데 한 뉴런의 출력이 그 다음 뉴련의 입력이라는 것은 한 뉴런이 계산하는 함수(내적과 비선형 활성화 함수의 조합)의 값을 그 다음 뉴런이 계산하는 함수의 인수로 사용한다는 의미. 이 점을 곰곰히 생각해보면 신경망 전체가 하나의 함성 함수임을 알 수 있고, 합성 함수는 미분의 연쇄 법칙으로 미분 가능.
  - $(f(g(x)))' = F'(x) = f'(g(x))g'(x)$
- 위 공식에 따라 뉴런에 주어진 입력에 대한 뉴런의 활성화 함수의 미분을 구할 수 있고, 그 미분이 있으면 해당 뉴런이 최종 오차에 얼마나 기여했는지 계산, 그에 따라 가중치를 적절히 갱신 가능.