# 2. 더 깊은 학습: 신경망 적용
## 5. 신경망 첫걸음: 퍼셉트론과 역전파
### 1. 신경망의 구성 요소
#### 4. 오차 곡면
- 앞에서 언급했듯이 신경망 훈련의 목표는 비용함수를 최소화 하는 것. 좀 더 구체적으로는 비용함수가 최소가 되는 최적의 매개변수(가중치)들을 구하는 것. 비용함수라는 것이 특정 자료점에 대한 오차와는 다른 것임을 명심. 우리가 하려는 것은 모든 입력에 대한 모든 오차를 망라한 비용을 최소화 하는 것.
- 신경망 학습 문제의 이러한 측면을 3차원 그래프로 시각화 시, 신경망의 가중치 갱신 과정을 이해하는 데 도움이 됨.
- 한 입력 견본에 대한 신경망의 오차는 입력 견본과 그 목푯값(바람직한 예측값) 이 얼마나 다른지를 나타냄. 둘의 차이를 그대로 사용하기보다는 제곱오차처럼 제곱한 값을 오차로 사용할 때가 많음. 모든 가능한 가중치 조합에 대해 그러한 오차값을 그래프로 그리면 일종의 다차원 곡면이 생김. 이 곡면에서 가장 낮은 점이 오차의 최솟값. 이를 최소점(minimal point)이라 함
- ![alt text](image-12.png)
- 이 최소점의 좌표 성분들은 주어진 훈련 견본에 대해 최적의 출력을 산출하는 가중치들. 가중치가 두 개일때는 이를 시각화하기 쉬움. 위의 그림과 비슷한 그래프는 두 가중치를 두 차원으로 두고 그에 해당하는 오차를 세번째 차원(높이)로 두어서 하나의 3차원 곡면을 표현. 실제 응용에서는 가중치가 훨씬 많으므로 훨씬 고차원의 곡면이 생기지만, 기본적 개념은 이 3차원의 경우와 다르지 않음.
- 마찬가지로, 비용함수도 이런식으로 곡면으로 시각화 가능. 비용함수는 특정 입력 견본에 대한 것이 아니라 입력 집합 전체에 대한 것이며, 흔히 쓰이는 것은 평균제곱오차(mean square error,MSE). 평균제곱오차는 말 그대로 제곱오차의 평균, 즉 모든 입력 견본에 대한 제곱오차의 합을 입력 견본의 수로 나눈 것. 가중치가 두 개라고 할 때 두 가중치를 $x$축과 $y$축으로 두고 평균제곱오차를 $z$축으로 두어서 비용함수의 곡면을 계산.
- 한 입력 견본에 관한 오차 곡면과 유사하게, 훈련 집합 전체에 대한 이러한 오차 곡면(비용 함수 곡면)의 최소점의 좌표성분들은 주어진 모형이 훈련 집합 전체에 대해 가장 좋은 성과를 내게 하는 가중치들.