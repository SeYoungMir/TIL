# 1.NLP 기초.
## 2. 단어 토큰화
### 2. 토큰 생성기를 이용한 어휘 구축
#### 5. 어휘 정규화
- 앞에서 보았듯이, 어휘의 크기는 NLP 파이프라인의 성능에 영향을 미침. 어휘 크기를 줄이는 또 다른 기법은 어휘를 정규화(normalization)하는 것, 즉 비슷한 토큰들을 하나의 정규화된 형태로 결합하는 것. 그러면 어휘의 토큰 개수가 줄어들 뿐만 아니라, 사실상 같은 의미이지만 '철자'가 다른 토큰 또는 n-그램들을 동일한 의미 단위로 취급할 수 잇게 됨. 그리고 이전에 언급했듯이, 어휘가 작아지면 과대적합이 일어날 가능성도 작아짐.
- 대소문자 합치기
  - 대소문자 합치기(case folding)는 대소문자 구성만 다른 단어들을 하나로 통합하는 것. 이런 대소문자 합치기가 필요한 이유는, 하나의 단어가 그 위치에 따라 또는 저자의 의도에 따라 대소문자 구성이 달라지기 때문. 문장의 첫 단어는 첫 글자를 대문자로 쓰는 것이 관례.
  - 또한, 강조를 위해 단어의 모든 글자를 대문자로 표기하기도 함. 불규칙한 대소문자 구성들을 하나의 구성으로 '정규화'한다는 점에서, 대소문자 합치기를 대소문자 정규화(case normalization)라고 부르기도 함. 단어와 문자의 대소문자 구성을 정규화하는 것은 어휘의 크기를 줄이고 NLP 파이프라인을 일반화하는 한 방법. 대소문자 합치기는 같은 의미를 가진(그리고 같은 철자로 표기해야 할) 여러 단어를 하나의 토큰으로 병합하는 데 도움이 됨.
  - 그런데 대소문자 구성이 나름의 의미를 지닐 때도 있음을 주의. 예를 들어 영어에서 'doctor'와 'Doctor'는 다른 뜻일 때가 많음. 일반적으로 대문자화(capitalization; 단어의 첫 글자를 대문자로 표기하는 것)는 인명, 지명, 제품명 같은 고유 명사를 표기할 때 흔히 쓰임. 만일 개체명 인식이 파이프라인의 주요 과제라면, 이런 고유 명사를 인식하는 능력이 중요. 따라서 단어들의 대소문자 구성을 유지하는 것이 유리. 그러나 대소문자 구성을 유지하면 어휘가 약 두 배로 커지며, 그러면 메모리와 처리 시간도 두 배가 됨. 그리고 기계 학습 파이프라인이 정확하고도 일반적인 해로 수렴하는 데 필요한 분류명 붙은(labeled) 훈련 자료의 양도 증가.
  - 다른 모든 기계 학습 파이프라인처럼, 훈련에 사용할 분류명 붙은 자료 집합은 반드시 모형이 다루어야 할 모든 가능한 특징 벡터들의 공간을 "대표할"수 있어야 함. NLP의 경우 그러한 벡터 공간을 "대표할" 수 있어야 함.
  - NLP의 경우 그러한 벡터 공간에는 한 단어의 여러 대소문자 구성이 포함. 예를 들어 차원 수가 10만인 단어 모음 벡터들로 이루어진 공간에서 지도 학습(supervised learning) 알고리즘으로 기계 학습 파이프라인을 훈련한다고 할 때, 과대적합을 피하려면 적어도 10만 개의 분류명 붙은 훈련 견본이 필요, 그보다 많아야 할 수도 있음. 때에 따라서는 어휘를 절반으로 줄여서 얻는 이득이 정보 내용의 손실보다 클 수 있음.
  - 파이썬에서는 목록 형성 기능을 이용해서 토큰들의 대소문자 구성을 손쉽게 정규화할 수 있음.
  - ```python
    >>> tokens = ['House','Visitor','Center']
    >>> normalized_tokens = [x.lower() for x in tokens]
    >>> print(normalized_tokens)
    ```
    