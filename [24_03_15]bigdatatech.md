# 6. 스크레이핑 개발(응용)
## 5. Scrapy를 사용해서 스크레이핑
### 1. 다양한 스크레이핑
- 크롤러를 개발하다보면 크롤러 개발에 어느 정도 패턴이 있다는 것을 알 수 있음.
- 대부분 링크가 모여있는 인덱스 페이지를 기반, 목표 링크들을 추출, 병렬적으로 컨텐츠 내려받음.
- 컨텐츠가 HTML 또는 XML이라면 XPath 또는 CSS 선택자를 사용해서 스크레이핑, 필요한 데이터 추출
- 이를 필요에 따라 XML과 JSON 등으로 저장/출력
- 인증이 필요한 경우에는 쿠키를 사용하기도 함. 레퍼러를 같은 사이트로 두어야 제대로 페이지를 받을 수 있는 경우도 있음.
- 프로그램 설계와 구현에 익숙하지 않다면 이러한 과정을 모두 처음부터 만들어 결합하는 것이 어려울 수 있음. 그러할 때는 프레임 워크를 사용하는 것이 편리
1. Scrapy란?
   - Scrapy는 크롤러 개발을 위한 프레임워크
   - 시작이 되는 인덱스를 설정, '어떠한 규칙에 따라 스크레이핑, 어떠한 모델에 맞게 데이터 추출, 출력한다'라는 방법을 규칙에 따라 적으면 자동으로 크롤링.
   - 내부적으로 병렬 다운로드 등을 지원, 크롤러의 세부적인 내부 구현 안해도 됨.
     - Scrapy의 내부에는 'Twitsted'라는 네트워크 프로그래밍 프레임워크 사용
   - Scrapy는 몇 가지 서브 프로젝트 가지고 있음.  예를 들어 'w3lib'은 HTML 태그 제거와 URL 인코드 등을 간편하게 해 주는 유틸리티. 필요에 따라서 이러한 것을 조합해서 사용
     - w3lib[(URL)](https://github.com/scrapy/w3lib)
   - 패턴에 맞지 않는 경우도 있음. 패턴에 맞지 않을 때는 크롤러 설계부터 모두 해야함.
   - 흐름을 이해하기 전까지는 Scrapy로 크롤러 개발 연습을 해보는 것도 좋음
   - Scrapy는 주로 다음과 같은 항목 사용
     - Item
       - 여러 개의 항목을 모아 놓은 모델. 예를 들어 도서 정보라면 도서 이름과 저자 이름등을 가지는 아이템 정의
     - Spider
       - 크롤러 프로그램 본체, 웹을 순회하면서 크롤링하는 프로그램을 '봇(bot)' 또는 '스파이더(spider)'라고 부르므로 Spider라는 이름이 붙음.
       - 인덱스 페이지에서 어떤 링크를 추출하고, 추출한 링크에서 어떠한 데이터를 스크레이핑 해야 하는지 작성해서 만듦