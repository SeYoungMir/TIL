# 1.NLP 기초.
## 4. 단어 빈도에서 의미 찾기: 의미 분석
### 8. 주제 벡터의 위력
#### 1. 의미 기반 검색
- NumPy로 행렬 곱셈 연산들을 병렬화 할 수 있지만 ,그렇다고 연산 횟수 자체가 줄지는 않음. 단지 병렬화한 만큼 속도가 빨라질 뿐. 근본적으로 정확한 의미 기반 검색을 위해서는 질의당 O(N)회의 내적 계산(곱셈과 덧셈으로 이루어진)이 필요. 간단히 말해서 말뭉치가 커지면 연산 횟수도 그에 정비례해서 커짐. 구글 검색의 커다란 뭉치 규모에서는, 심지어 위키 의미 기반 검색의 말뭉치 규모에서도, 이런 O(N) 알고리즘이 통하지 않음
- 현실적인 우회책은 완벽한 색인을 만들려 하거나 정확한 검색 결과를 얻으려 하는 대신 "충분히 좋은" 결과를 추구하는 것. 예를 들어 LSH와 함께 근사 최근접 이웃(approximate nearest neighbor)알고리즘을 이용하면 큰 말뭉치에 대한 효율적인 의미 기반 검색이 가능. 그리고 근사 최근접 이웃 알고리즘을 효율적이고 정확하게 구현한 오픈소스 패키지들도 여러 개 있음. 사용과 설치가 쉬운 패키지 두 개는 다음과 같음.
  - Spotify의 Annoy 패키지
  - gensim의 gensim.models.KeyedVector 클래스
- 엄밀히 말해서 이런 색인화 또는 해싱 해법들에 기초한 의미 기반 검색이 주어진 질의문에 가장 잘 부합하는 문서를 찾아낸다는 보장은 없음. 그렇지만 이런 기법을 이용하면 "충분히 좋은"검색 결과들을 TF-IDF 벡터나 단어 모음 벡터에 대한 전통적인 역색인에 기초한 검색 엔진만큼이나 빨리 얻을 수 있음. 정확성을 조금 희생하는 대신 커다란 속도 향상을 획득.
#### 2. 개선안
- 이후에는 이번에 이야기한 주제 벡터 개념을 좀 더 조율, 단어들과 연관된 벡터를 더욱 정밀하고 유용하게 만듦. 그러한 개선의 비결은 신경망과 심층 학습. 그것이 다음 파트 전체의 주제. 다음 파트의 처음 장인 5장에서는 신경망의 기초를 이야기. 신경망을 파이프라인에 도입 시 짧은 텍스트 조각들에서 심지어는 개별 단어들에서 의미 추출도 가능함.