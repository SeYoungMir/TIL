# 1.NLP 기초.
## 2. 단어 토큰화
### 2. 토큰 생성기를 이용한 어휘 구축
#### 4. n-그램을 이용한 어휘 확장
- 불용어
  - 어휘가 커지면 임의의 특정 단어 또는 단어 조합에 대한 과대적합을 피하기 위해 필요한 훈련 자료 집합의 크기도 커진다는 점을 걱정, 게다가 훈련 집합이 커지면 전체적인 처리량도 커짐. 그러나 2만 단어 어휘에서 불용어 100개를 제거한다고 처리 속도가 크게 빨라지는 것은 아님.
  - 그리고 2-그램 어휘의 경우, 텍스트의 불용어들을 포함한 2-그램들의 빈도를 점검하지 않고 무작정 불용어들을 제거해서 얻는 이득은 무시해도 될 정도로 작음.
  - 예를 들어 정관사"The"를 제거 시 2-그램을 "The Shining"을 음산한 분위기의 공포 영화 제목으로 인식하지 못해, 이런 2-그램이 담긴 문서를 그냥 "Shining Light" 혹은 "shoe shining" 같은 단어 조합이 있는 문서와 마찬가지로 취급.
  - 메모리와 처리 능력이 충분해서 큰 어휘에 대해 파이프라인의 모든 단계를 너끈히 실행할 수 있다며 여기저기 등장하는 몇 개의 중요하지 않은 단어들을 그리 걱정할 필요가 없음. 그리고 어휘가 크지만 훈련 집합이 작아서 과대적합이 걱정된다면, 불용어를 제거하는 것보다는 어휘를 좀 더 잘 선택하거나 차원 수를 줄이는 것이 더 나은 방법.
  - 불용어를 어휘에 포함하면, 문서 빈도 필터(다음장에서 논의)가 주어진 응용 영역의 주요 정보 내용을 그리 많이 담고 있지 않은 단어들과 n-그램들을 좀더 잘 식별, 제외함
  - 토큰화 과정에서 일단의 불용어들을 무조건 제거하기로 했다면, 그냥 파이썬의 목록 형성(list comprehension)기능으로 간단하게 불용어 필터를 구현 가능. 다음은 토큰 목록에서 영어 불용어 몇 개를 제거하고 그 나머지 토큰을 출력하는 예
    - ```python
      >>> stop_words = ['a','an','the','on','of','off','this','is']
      >>> tokens = ['the','house','is','on','fire']
      >>> tokens_without_stopwords=[x for x in tokens if x not in stop_words]
      >>>print(tokens_without_stopwords)
      ```
      