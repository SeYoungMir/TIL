# 1.NLP 기초.
## 2. 단어 토큰화
### 2. 토큰 생성기를 이용한 어휘 구축
#### 4. n-그램을 이용한 어휘 확장
- 다음 장에서는 이런 n-그램 중 다른 것들에 비해 더 많은 정보를 담고 있는 것들을 식별함으로써 NLP 파이프라인이 관리할 토큰(n-그램)들을 더욱 줄이는 기법들을 설명. 그런 기법이 없으면 파이프라인은 모든 n-그램을 저장하고 처리해야 함.
- 예를 들어 "Thomas Smith"나 "ice shatterd" 보다 "Thomas Jefferson"과 "ice cream"에 더 큰 점수를 부여하는 방법을 다음장에서 소개.
- 그 다음장에는 두 단어 쌍(2-그램)에 각 구성 단어의 의미와는 독립적으로 또 다른 의미를 부여하는 방법도 논의. 이런 방법은 2-그램보다 훨씬 긴 순차열에 대해서도 가능. 그러나 이런 모든 기법을 위해서는 먼저 토큰 생성기가 n-그램들을 생성할 수 있어야 함.
- 이번에도 토머스 제퍼슨에 관한 문장을 예로 사용. 우리의 목표는 두 단어들로 이루어진 토큰을 얻는 것.
- 문장을 개별 단어들로 토큰화할 때보다 2-그램들로 토큰화할 때 훨씬 많은 정보가 유지됨을 알 수 있음. NLP 파이프라인의 이후 단계들은 이 토큰화 단계가 제공한 토큰들에만 접근. "Thomas"가 농구선수 토마스나 기관차 토마스를 말하는 것이 아님을 이후 단계들이 알게 하는 한 방법이 n-그램
- 정리하자면 n-그램 토큰화는 파이프라인을 통해서 자료와 함께 그 문맥 정보도 전달하는 하나의 수단.
- 다음은 nltk의 n-그램 토큰화 함수를 사용하는 예
- ```python
  >>> from nltk.utill import ngrams
  >>> list(ngrams(tokens,2))
  >>> list(ngrams(tokens,3))
  ```
  - 메모리 효율성을 위해 NLTK 라이브러리의 ngrams 함수는 파이썬의 생성기(generator)객체를 돌려줌. 파이썬의 생성기 객체는 마치 반복자(iterator)처럼 작동하는 "똑똑한" 함수. 모든 n-그램의 순차열을 한 번에 돌려주는(return)것이 아니라 한 번에 하나씩만 '산출'함(yield)
  - 이러한 생성기는 for 루프에서 유용. 생성기를 이용 시 순차열의 모든 요소를 메모리에 담아둘 필요 없이 개별 항목을 하나씩만 가져와서 처리 가능. 반환된 n-그램들을 한꺼번에 또는 임의의 순서로 조사하고 싶으면, 예제처럼 생성기가 산출한 결과들을 하나의 목록(list)로 만들면 됨.
  - 단 , 이런 접근 방식은 지금처럼 토큰화를 대화식 세션에서 수행할 때나 바람직함, 커다란 문서에 대한 실제 토큰화 작업(시간이 오래 걸릴 수 있는)에는 바람직하지 않음을 주의.