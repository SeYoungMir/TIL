# 2. 더 깊은 학습: 신경망 적용
## 5. 신경망 첫걸음: 퍼셉트론과 역전파
### 1. 신경망의 구성 요소
#### 4. 경사로 따라 활강.
- 이러한 시각화를 좀 더 해석. 각 세(훈련 주기)에서 훈련 알고리즘은 경사 하강법(gradient descent)이라고 부르는 알고리즘을 이용해서 오차를 최소화하려 함. 경사 하강법의 개념은 간단. 이름 그대로 경사로를 따라 아래로 (즉, 오차가 줄어드는 기울기 방향으로) 내려가다 보면 최소점에 도달. 오차 곡면이 이전 그림처럼 매끄러운 볼록 곡면이면, 결국에는 최소점에 도달.
- 그러나 오차 곡면이 항상 매끄러운 볼록 곡면이라는 보장이 없음. 다양한 구덩이와 봉우리, 계곡이 존재. 그런 모양을 비볼록(nonconvex) 오차곡면이라고 함. 구덩이가 충분히 크면 구덩이를 벗어나지 못해 경사면의 최하단에 도달하지 못하게 됨.
- 다음 이미지에 비볼록 곡면의 예시.
- ![alt text](image-13.png)
- 시각화를 위해 2차원 입력에 대한 가중치들을 축으로 두었으나, 입력이 10차원이나 50차원, 1000차원이라고 해도 개념은 동일. 더 고차원의 공간에서는 이런 식의 시각화가 사실상 불가능, 수학에 의존할 수 밖에 없음. 실제로 신경망을 개발하고 사용하기 시작하면 오차 곡면의 시각화는 덜 중요. 그냥 관련 측정치들로도 오차 곡면을 시각화해서 얻는 것과 동일한 정보를 얻을 수 있음. 중요한 것은 훈련 과정에서 오차가 0을 향해 점차 줄어들고 있느냐인데, 그 여부는 신경망이 제대로 학습하고 있는지 아닌지를 말해 줌. 
- 비볼록 오차 공간의 구덩이들은 어떻게 극복하는가? 가중치들을 어떻게 초기화하느냐에 따라 활강의 시작점이 달라지며, 활강을 어딧허 시작하느냐에 따라서는 구덩이에 빠져서 진짜 최소점에 도달하지 못하고 극소점(국소 최소점)에서 활강 종료.
- 3차원 뿐만 아니라 더 높은 차원의 오차 공간에도 이런 극소점들이 존재.