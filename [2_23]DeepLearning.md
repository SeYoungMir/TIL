### 딥러닝(DeepLearning)
- 미지의 일을 예측하는 힘
  - 기존 환자의 데이터를 이용해 새로운 환자의 생사를 예측하는 프로그램
  - 머신러닝을 이용해야함
    - 기존의 프로그래밍은 데이터를 입력해서 답을 구한다. 
    - 머신러닝은 데이터와 답을 입력해서 규칙을 찾는다. 이 규칙을 다른 데이터에도 적용! (예측
    - 미지의 일을 예측하기 위해 서는 기존 프로그래밍 방식이 아니라 머신 러닝 기법을 써야함
  - 기존 환자의 데이터는 머신러닝에 입력되는 순간, 그 패턴과 규칙이 분석됨 → 이를 학습(training) 이라고 함
  - 분석 결과를 새로운 환자의 데이터와 비교해 생존 가능성이 몇 퍼센트인지알려 줌
    - 이것이 바로 머신러닝이 하는 일
- 예시: 깨끗한 좌표 평면에 기존 환자들을 하나씩 배치하는 과정
  -  환자들의 분포를 그래프 위에 펼쳐 놓고
  -  이 분포도 위에 생존과 사망 여부를 구분짓는 경계를 그려넣음. 
  - 이를 잘 저장해 놓았다가 새로운 환자가 오면 분포도의 어디쯤 위치하는지를 파악
  - 머신러닝의 예측 성공률은 결국 얼마나 정확한 경계선을 긋느냐에 달려 있다
#### Classification

- 분류의 문제 = 선긋기(기준 찾기)
- 과일 분류하기(수확 가능여부 확인)
- 고정 기준 = if (크기 > 9 ) then 수확 else 미수확
- 기준 찾기 = Learning(or Training Data Set iteration
- 크기를 기준으로 생성한 모델을 Test Data Set에 적용
- Training Data Set에서 높은 정확도를 보여도 Test Data Set의 결과는 부적합
- 색의 진하기를 분류 기준으로 추가

#### Neural Network
- 인공 신경망
- 기계 학습 분야에서 연구되는 학습 알고리즘
- 시계열 자료의 예측, 분류, 패턴 인식, 제어 분야에 응용
- 인간의 뇌 구조를 모방하여 만들어짐(신경세포:Neuron)
- 수상돌기(Dendrite), 신경세포체(Soma), 축색(Axon), 시냅스(Synapse)
- 수상돌기 = 입력, 축색 = 출력
- 신경세포체 = 노드, 시냅스 = 가중값을 갖는 연결 네트워크

#### Multi Layer Perceptron(MLP)
- 다층 퍼셉트론(or Deep Neural Network)
- 퍼셉트론으로 해결할 수 없는 비선형 분리 문제에 필요
- 여러 층의 퍼셉트론을 쌓아서 동작
- 직선을 여러 번 그어서 **비선형 분리 문제**를 해결
- model.add(input)
- model.add(layer1)
- model.add(layer2)
- model.add(output)
#### Artificial Neural Network(ANN)
- Handwritten digits in the M(Mixed)-NIST database
- 1000개의 이미지 테스크 결과(92.2% 정확도)

#### Wrap Up

- 데이터 저장 및 공유 기술의 발달로 신경망 학습에 필요한 데이터 제공 증가
- GPU(Graphics Processing Unit)를 사용하여 컴퓨팅 파워 증가
- 개선된 알고리즘
- 인공 신경망은 샘플 크기가 커야 함(많은 학습 데이터 필요)
- 서로 다른 활성화 규칙에 따라 여러 개의 레이어로 구성되어 해석이 어려움
- 역전파(Backpropagation) : 예측 정확도에 따라 활성화 규칙을 정제
- 인공 신경망은 데이터가 크고 고성능 컴퓨팅 파워를 사용 가능할 때 적합
### 신경망으로 딥러닝
#### 퍼셉트론
- 인간의 뇌는 치밀하게 연결된 약 1000억 개의 뉴런으로 이루어져 있음
- 뉴런과 뉴런 사이에는 시냅스라는 연결 부위가 있는데, 신경 말단에서 자
극을 받으면 시냅스에서 화학 물질이 나와 전위 변화를 일으킴
- 전위가 임계 값을 넘으면 다음 뉴런으로 신호를 전달하고, 임계 값에 미치지 못하면 아무것도 하지 않음 → 퍼셉트론의 개념과 유사!
- 신경망을 이루는 가장 중요한 기본 단위는 퍼셉트론(perceptron)
- 퍼셉트론은 입력 값과 활성화 함수를 사용해 출력 값을 다음으로 넘기는 가장 작은 신경망 단위

#### 가중치, 가중합, 바이어스, 활성화 함수
- 기울기 a나 y 절편 b와 같은 용어를 퍼셉트론의 개념에 맞춰 좀 더 ‘딥러닝
답게’ 표현해 보면 다음과 같음
- 먼저 기울기 a는 퍼셉트론에서는 가중치를 의미하는 w(weight)로 표기됨
- y 절편 b는 똑같이 b라고 씀, 하지만 y = ax + b의 b가 아니라 편향, 선입견이라는 뜻인 바이어스(bias)에서 따온 b
- 가중합(weighted sum): 입력 값(x)과 가중치(w)의 곱을 모두 더한 다음 거기에 바이어스(b)를 더한 값
- 가중합의 결과를 놓고 1 또는 0을 출력해서 다음으로 보냄
- 여기서 0과 1을 판단하는 함수가 있는데, 이를 활성화 함수(activation function) 라고 함. 앞서 배웠던 시그모이드 함수가 바로 대표적인 활성화함수

#### 다층 퍼셉트론

- XOR 문제를 해결하기 위해서 두 개의 퍼셉트론을 한 번에 계산할 수 있어
야 함
- 이를 가능하게 하려면 숨어있는 층, 즉 은닉층(hidden layer)을 만들면 됨
- 입력층과 은닉층의 그래프를 집어넣어 보면 은닉층이 좌표 평면을 왜곡시키는 결과를 가져옴 → 두 영역을 가로지르는 선이 직선으로 바뀜
- 다층 퍼셉트론이 입력층과 출력층 사이에 숨어있는 은닉층을 만드는 것을
도식
- 가운데 숨어있는 은닉층으로 퍼셉트론이 각각 자신의 가중치(w)와 바이어스(b) 값을 보내고, 이 은닉층에서 모인 값이 한 번 더 시그모이드 함수($\sigma$ 기호로 라고 표시)를 이용해 최종 값으로 결과를 보냄
- 은닉층에 모이는 중간 정거장을 노드(node)라고 하며, 여기서는 $n_1$과 $n_2$로 표현

#### 오차 역전파
- 그러다 보니 최적화의 계산 방향이 출력층에서 시작해 앞(뒤어서 앞으로)
으로 진행됨 : 오차 역전파(back propagation)라고 부름
- 오차 역전파 구동 방식의 정리
  1) 임의의 초기 가중치(w(1))를 준 뒤 결과(yout)를 계산한다. 
  2) 계산 결과와 우리가 원하는 값 사이의 오차를 구한다. 
  3) 경사 하강법을 이용해 바로 앞 가중치를 오차가 작아지는 방향으로 업데이트한다. 
  4) 1~3 과정을 더이상 오차가 줄어들지 않을 때까지 반복한다.
- 여기서 ‘오차가 작아지는 방향으로 업데이트한다’는 의미는 미분 값이 0에
가까워지는 방향으로 나아간다는 말
- 즉, ‘기울기가 0이 되는 방향’으로 나아가야 하는데, 이 말은 가중치에서 기울기를 뺐을 때 가중치의 변화가 전혀 없는 상태를 말함
- 따라서 오차 역전파를 다른 방식으로 표현하면 가중치에서 기울기를 빼도 값의 변화가 없을 때까지 계속해서 가중치 수정 작업을 반복하는 것
#### 오차 역전파 문제
-다층 퍼셉트론이 오차 역전파를 만나 신경망이 되었고, 신경망은 XOR 문
제를 가볍게 해결
- 하지만 기대만큼 결과가 좋아지지 않았음
- 이유가 무엇일까?
#### 기울기 소실 문제와 활성화 함수
- 기울기 소실 문제!
- 오차 역전파는 출력층으로부터 하나씩 앞으로 되돌아가며 각 층의 가중치를 수정하는 방법
- 가중치를 수정하려면 미분 값, 즉 기울기가 필요하다고 배움
- 그런데 층이 늘어나면서 기울기가 중간에 0이 되어버리는 기울기 소실(vanishing gradient) 문제가 발생하기 시작

- 이는 활성화 함수로 사용된 시그모이드 함수의 특성 때문임
- 시그모이드를 미분하면 최대치가 0.3
- 1보다 작으므로 계속 곱하다 보면 0에 가까워짐
- 따라서 층을 거쳐 갈수록 기울기가 사라져 가중치를 수정하기가 어려워지는 것
- 이를 해결하고자 활성화 함수를 시그모이드가 아닌 여러 함수로 대체하기시작 (시그모이드, 하이퍼볼릭 탄젠트, 렐루, 소프트플러스)
- 토론토대학교의 제프리 힌튼 교수가 제안한 렐루(ReLU)는 시그모이드의
대안으로 떠오르며 현재 가장 많이 사용되는 활성화 함수
- 렐루는 x가 0보다 작을 때는 모든 값을 0으로 처리하고, 0보다 큰 값은 x를그대로 사용하는 방법. 이 방법을 쓰면 x가 0보다 크기만 하면 미분 값이 1이 됨
- 따라서 여러 은닉층을 거치며 곱해지더라도 맨 처음 층까지 사라지지 않고 남아 있을 수 있음: 딥러닝의 발전에 속도가 붙게 됨

#### 속도와 정확도 문제를 해결하는 고급 경사 하강법
- 속도와 정확도 문제!
- 경사 하강법은 정확하게 가중치를 찾아가지만, 한 번 업데이트할 때마다
전체 데이터를 미분해야 하므로 계산량이 매우 많다는 단점이 있음
- 확률적 경사 하강법(SGD)
  - 전체 데이터를 사용하는 것이 아니라, 랜덤하게 추출한 일부 데이터를 사용
  - 일부 데이터를 사용하므로 더 빨리 그리고 자주 업데이트를 하는 것이 가능해짐
- 모멘텀
  - 경사 하강법과 마찬가지로 매번 기울기를 구하지만, 이를 통해 오차를 수정하기 전 바로 앞 수정 값과 방향(+, -)을 참고하여 같은 방향으로 일정한비율만 수정되게 하는 방법 (이도에 탄력을 더한다)
- 딥러닝 구동에 필요한 고급 경사 하강법과 케라스 내부에서의 활용법 정리
<table>
    <tr>
        <th>고급 경사 하강법</th>
        <th>개요</th>
        <th>효과</th>
        <th>케라스 사용법</th>
    </tr>
    <tr>
        <td>1.확률적 경사 하강법(SGD)</td>
        <td>랜덤하게 추출한 일부 데이터를 사용해 더 빨리, 자주 업데이트를 하게 하는 것</td>
        <td>속도 개선</td>
        <td>keras.optimizers.SGD(lr=0.1)케라스 최적화 함수를 이용합니다.</td>
    </tr>
    <tr>
    <td>2.모멘텀(Momentum)</td>
    <td>관성의 방향을 고려해 진동과 폭을 줄이는 효과</td>
    <td>정확도 개선</td>
    <td>keras.optimizers.SGD(lr=0.1,momentum=0.9)모멘텀 계수를 추가합니다.</td>
    </tr>
    <tr>
        <td>3.네스테로프 모멘텀(NAG)</td>
        <td>모멘텀이 이동시킬 방향으로 미리 이동해서 그레디언트를 계산, 불필요한 이동을 줄이는 효과</td>
        <td>정확도 개선</td>
        <td>keras.optimizers.SGD(li=0.1,momentum=0.9,nesterov=True)네스테로프 옵션을 추가합니다.</td>
    </tr>
    <tr>
        <td>4.아다그라드(Adagrad)</td>
        <td>변수의 업데이트가 잦으면 학습률을 적게 하여 이동 보폭을 조절하는 방법</td>
        <td>보폭 크기 개선</td>
        <td>keras.optimizers.Adagrad(lr=0.01,epsilon=1e-6)아다그라드 함수를 사용합니다.
        @참고 : 여기서 epsilon,rho,decay같은 파라미터는 바꾸지 않고 그대로 사용하기를 권장, 따라서 lr,즉 learning rate(학습률)값만 적절히 조정</td>
    </tr>
    <tr>
        <td>5.알엠에스 프롭(RMSProp)</td>
        <td>아다그라드의 보폭 민감도를 보완한 방법</td>
        <td>보폭 크기 개선</td>
        <td>keras.optimizers.RMSprop(lr=0.001,rho=0.9,epsilon=1e-08,decay=0.0)알엠에스프롭 함수를 사용합니다.
        </td>
    </tr>
    <tr>
        <td>6.아담(Adam)</td>
        <td>모멘텀과 알엠에스프롭 방법을 합친 방법</td>
        <td>정확도와 보폭 크기 개선</td>
        <td>keras.optimizers.Adam(lr=0.001,beta_1=0.9,beta_2=0.999,epsilon=1e-08,decay=0.0)아담 함수를 사용합니다.</td>
    </tr>
</table>

#### 딥러닝 모델링
- 준비
  - import tensorflow,keras
- 데이터 준비
  - 1. 데이터 로드
  - 2. feature, label
- 모델 구조 결정
  - 1. segment
  - 2. add()
- 모델 실행
  - compile(metrics=acurracy)
  - fit()
- 모델 검증
  - 실제 Data
  - 예측 Data 비교
  - evaluate()

#### 폐암 수술 환자의 생존율 예측하기

- 딥러닝은 여러 층이 쌓여 결과를 만들어 냄
- Sequential 함수는 딥러닝의 구조를 한 층 한 층 쉽게 쌓아올릴 수 있게 해 줌
- Sequential 함수를 선언하고 나서 model.add() 함수를 사용해 필요한 층을차례로 추가하면 됨
- model.add() 함수를 이용해 두 개의 층을 쌓아 올렸음
```python
model = Sequential()
model.add(Dense(30, input_dim=16, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
```
- 층을 몇 개 쌓을지는 데이터에 따라 그때 그때 결정
- 케라스의 가장 큰 장점 중 하나는 model.add() 함수를 이용해 필요한 만큼의 층을 빠르고 쉽게 쌓아 올릴 수 있다는 것
- model.add() 안에는 Dense() 함수가 포함되어 있음
  - 영어 단어 dense는 ‘조밀하게 모여있는 집합’이란 뜻, 여기서는 각 층이 제각각 어떤 특성을 가질지 옵션을 설정하는 역할을 함
- 딥러닝의 구조와 층별 옵션을 정하고 나면 compile() 함수를 이용해 이를 실행시킴
- 여기에 쓰인 loss, optimizer, activation 등의 키워드를 이해하는 것이 바로 딥러닝 학습의 핵심
- 출력 부분에서는 model.evaluate() 함수를 이용해 앞서 만든 딥러닝의 모델이
어느 정도 정확하게 예측하는지를 점검할 수 있음
- 이 코드를 통해 출력되는 정확도(Accuracy)는 학습 대상이 되는 기존 환자들의데이터 중에 일부를 랜덤하게 추출해, 새 환자인 것으로 가정하고 테스트한 결과 좀 더 신뢰할 수 있는 정확도를 측정하려면, 학습 단계에서 미리 일부를 떼어내어 테스트셋으로 저장하고 테스트할 때는 오직 이 테스트셋만을 사용

### 모델의 설계
#### 모델의 정의
- 딥러닝의 모델을 설정하고 구동하는 부분은 모두 model이라는 함수를 선
언하며 시작이 됨
- model = Sequential()로 시작되는 부분은 딥러닝의 구조를 짜고 층을 설정하는 부분
- model.compile() 부분은 위에서 정해진 모델을 컴퓨터가 알아들을 수 있게끔 컴파일 하는 부분
- model.fit()으로 시작하는 부분은 모델을 실제로 수행하는 부분

#### 입력층, 은닉층, 출력층
- Sequential() 함수를 model로 선언해 놓고 model.add()라는 라인을 추가하면 새로운 층이 만들어짐
  - 코드에는 model.add()로 시작되는 라인이 두 개가 있으므로 두 개의층을 가진 모델을 만든 것
- 맨 마지막 층은 결과를 출력하는 ‘출력층’이 됨
- 나머지는 모두 ‘은닉층’의 역할을 함
- 각각의 층은 Dense라는 함수를 통해 구체적으로 그 구조가 결정됨

- Dense(30, input_dim=16, activation='relu') 
  - 이 층에 몇 개의 노드를 만들 것인지를 결정
  - 30이라고 되어 있는 것은 이 층에 30개의 노드를 만들겠다는 것
- input_dim 변수는 입력 데이터로부터 몇 개의 값이 들어올지를 정하는 것
- keras는 입력층을 따로 만드는 것이 아니라, 첫 번째 은닉층에 input_dim을 적어 줌으로써 첫 번째 Dense가 은닉층 + 입력층의 역할을 겸함
- 폐암 수술 환자의 생존 여부 데이터에는 16개의 입력 값들이 있음
  - 데이터에서 16개의 값을 받아 은닉층의 30개 노드로 보낸다는 뜻

- 첫 번째 Dense는 입력층과 첫 번째 은닉층을,두 번째 Dense는 출력층을 의미
- 이제 은닉층의 각 노드는 16개의 입력 값으로부터 임의의 가중치를 가지
고 각 노드로 전송되어 활성화 함수를 만남
- 그리고 활성화 함수를 거친 결과 값이 출력층으로 전달됨
- 이때 활성화 함수로 무엇을 사용할지 지정해야 함 (activation 부분에)
- 두 번째 나오는 model.add(Dense(1,activation='sigmoid'))는 마지막 층이
므로 이 층이 곧 출력층이 됨
- 출력 값을 하나로 정해서 보여 줘야 하므로 출력층의 노드 수는 1개
- 이 노드에서 입력받은 값 역시 활성화 함수를 거쳐 최종 출력 값으로 나와야 함
- 여기서는 시그모이드(sigmoid)를 활성화 함수로 사용

#### 모델 컴파일
- 다음으로 model.compile 부분
```python
model.compile(loss=' binary_crossentropy ', optimizer='adam', metrics=['accuracy'])
```
- model.compile 부분은 앞서 지정한 모델이 효과적으로 구현될 수 있게 여러 가지 환경을 설정해 주면서 컴파일하는 부분

- 먼저 어떤 오차 함수를 사용할지를 정해야 함
  - binary_crossentropy(이항 교차 엔트로피)를 사용
참고) 평균 제곱 오차 함수(mean_squared_error)

- 최적화를 위해 아담(adam)을 사용. 

#### 교차 엔트로피
- 오차 함수에는 평균 제곱 오차 계열의 함수 외에도 교차 엔트로피 계열의 함수가 있음
  - 평균 제곱 오차는 수렴하기까지 속도가 많이 걸린다는 단점이 있다
  - 교차 엔트로피는 출력 값에 로그를 취해서, 오차가 커지면 수렴 속도가 빨라지고 오차가 작아지면 속도가 감소하게끔 만든 것
- 교차 엔트로피는 주로 분류 문제에서 많이 사용
  - 예측 값이 참과 거짓 둘 중 하나인 형식일 때는 binary_crossentropy(이항 교차 엔트로피)를 씀
  - binary_crossentropy를 적용하려면 앞 식의 mean_squared_error 자리에 binary_crossentropy를 입력하면 됨

- 케라스에서 사용되는 대표적인 오차 함수
  - 실제 값을 yt, 예측값을 yo라 가정하자
<table>
    <tr>
        <th rowspan=4>평균 제곱 계열</th>
        <td>mean_squared_error</td>
        <td>평균 제곱 오차<br>계산:mean(square(yt-yo))</td>
    </tr>
    <tr>
        <td>mean_absoulute_error</td>
        <td>평균 절대 오차(실제 값과 예측값 차이의 절댓값 평균)<br>계산:mean(abs(yt-yo))
    </tr>
        <td>mean_absolute_percentage_error</td>
        <td>평균 절대 백분율 오차(절댓값 오차를 절댓값으로 나눈 후 평균)<br>계산: mean(abs(yt-yo))/abs(yt)(단,분모 &#8800; 0)</td>
    <tr>
        <td>mean_squared_logarithmic_error</td>
        <td>평균 제곱 로그 오차(실제 값과 예측 값에 로그를 적용한 값의 차이를 제곱한 값의 평균)<br>계산: mean(square((log(yo)+1)-(log(yt)+1)))
    </tr>
    <tr>    
        <th rowspan=2>교차 엔트로피 계열</th>
        <td>categorical_crossentropy</td>
        <td>범주형 교차 엔트로피(일반적인 분류)</td>
    </tr>
    <tr>
        <td>binart_crossentropy</td>
        <td>이항 교차 엔트로피(두 개의 클래스 중에서 예측할 때)</td>
    </tr>
</table>

#### 모델 실행하기
- 모델의 실행: model.fit()
```python
model.fit(X, y, epochs=30, batch_size=16)
```
- 학습 프로세스가 모든 샘플에 대해 한 번 실행되는 것을 1 epoch(에포크)
- Epochs=30으로 지정한 것은 각 샘플이 처음부터 끝까지 30번 재사용될때까지 실행을 반복하라는 뜻
- batch_size는 샘플을 한 번에 몇 개씩 처리할지를 정하는 부분
- batch_size=16은 전체 470개의 샘플을 16개씩 끊어서 집어넣으라는
  - batch_size가 너무 크면 학습 속도가 느려지고, 너무 작으면 각 실행값의 편차가 생겨서 전체 결괏값이 불안정해질 수 있음
  - 자신의 컴퓨터 메모리가 감당할 만큼의 batch_size를 찾아 설정
