# 1.NLP 기초.
## 4. 단어 빈도에서 의미 찾기: 의미 분석
### 4. 주성분 분석(PCA)
#### 1. 3차원 벡터에 대한 PCA
- 점 구름을 적절히 회전, 각 축에 대한 분산이 최소가 되는 방향을 탐색. 그런 방향을 찾기까지는 점 구름을 여러 방향으로 회전해 봐야 함.
- SVD(LSA)로 문서-단어 벡터들을 그런식으로 회전한다고 하면, 그런 벡터들에 대한 정보가 '감추어졌을'것.
- 분산이 최소가 되는 방향에서 3차원 점들을 2차원 평면으로 투영 시 여러 개의 점이 겹쳐지며, 따라서 사람이 눈으로 보고(그리고 기계 학습 알고리즘이 수치들을 파악해서)그 점들을 제대로 분리하기 어려움. SVD는 고차원 공간을 저차원에 투영한 '그림자'의 차원(축)들에 대한 분산이 최대화되는 방향을 탐색, 벡터들의 구조와 정보 내용을 보존. 기계 학습에서 각각의 저차원 벡터가 그것이 표현하는 고차원 자료의 '본질'을 포착하려면 그런 방식의 처리가 필요.
- SVD는 각 축에 대한 분산을 최대화. 그리고 분산은 우리가 원하는 '정보'또는 '본질'을 잘 대표하는 수치.
- ```python
  import pandas as pd
  pd.set_option('display.max_columns',6)
  from sklearn.decomposition import PCA
  import seaborn
  from matplotlib import pyplot as plt
  from nlpia.data.loaders import get_data
  df = get_data('pointcloud').sample(1000)
  pca = PCA(n_components=2)
  df2d= pd.DataFrame(pca.fit_transform(df),columns=list('xy'))
  df2d.plot(kind='scatter',x='x',y='y')
  plt.show()
  ```
- 위 스크립트 시행시 2차원 투영('그림자')이 좌우로 '뒤집힐' 수 있음. 그러나 투영의 방향이 다른 각도로 회전하지는 않음. scikit-learn의 PCA는 항상 2차원 투영의 방향을 최대 분산이 첫 축인 x축에 정렬되도록 계산. 그리고 두번째로 큰 분산은 둘째 축인 y축과 정렬. 그러나 이 축들의 극성(polarity) 또는 부호는 임의적. 이는 PCA의 방향 최적화에 2의 자유도가 남아 있기 때문. 즉, 방향 최적화는 벡터(자료점)들의 극성을 x축이나 y축, 또는 두 축 모두에 대해 뒤집을 수 있음.
- 자료점들의 3차원 방향을 직접 조작해보고 싶다면 nlpia/data 디렉터리의 horse_plot.py 스크립트를 사용. 자료 정보 내용을 감소하지 않고도 (눈으로 보기에) 차원을 하나 더 죽이는 방향을 탐색 가능. 더 나아가서 피카소와도 같은 자료를 여러 관점에서 동시에 보면서 정보 내용을 유지하는 비선형 변환을 탐색 가능. 컴퓨터에도 이런 일을 수행하는 '내장(embedding)'알고리즘이 존재.
- 다른 알고리즘도 존재, SVD와 PCA도 자료저믜 '본질'을 잘 포착하는 방향을 능숙하게 탐색. 다음 그림을 보면 이 자료점들이 말을 스캔한 결과임을 알 수 있음. 컴퓨터가 3차원 벡터들에 담긴 통계적 특징을 활용, 말의 형태를 잘 보여주는 2차원 벡터들을 생성.
- ![alt text](image-8.png)