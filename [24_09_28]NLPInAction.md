# 1.NLP 기초.
## 2. 단어 토큰화
### 2. 토큰 생성기를 이용한 어휘 구축
#### 4. n-그램을 이용한 어휘 확장
- 예제가 산출한 각 n-그램은 하나의 튜플(tuple), 한 n-그램의 모든 토큰을 연결해 하나의 문자열로 만드는 것도 가능, 이처럼 각 n-그램을 하나의 문자열로 만들면 파이프라인의 이후 단계들이 일관되게 문자열(들의 순차열)을 입력으로 받으므로 설계가 단순화.
- 다음은 각 튜플을 문자열로 변환하는 예제
- ```python
  >>> two_grams = list(ngrams(tokens,2))
  >>> [" ".join(x) for x in two_grams]
  ```
- 이 부분의 문제점으로, 상식적으로 "Thomas Jefferson"이라는 하나의 2-그램은 꽤 많은 영어 문서에 등장, "of 26",이나 "Jefferson began"이 출연하는 문서는 극히 드뭄. 어떤 토큰이나 n-그램이 극히 드물게만 나타난다는 것은 그 토큰이 다른 단어들(일단의 문서들을 연결해주는 어떤 주제나 화제를 식별하는 데 도움이 되는)과 상관관계가 거의 없다는 의미.
- 즉 희소한 n-그램은 분류 문제에 그리 도움이 되지 않으며, 대부분의 2-그램은 상당히 드물고, 3-그램이나 4-그램은 그보다 더 드묾.
- 단어 조합이 개별 단어보다 희소하믈, 어휘의 크기는 말뭉치의 모든 문서에 있는 n-그램들의 수에 지수적으로 접근. 특징 벡터의 차원 수가 문서의 길이보다 크면 특징 추출 단계가 오히려 파이프라인의 생산성에 해를 미칠 수 있음
- 이 경우, 기계 학습 모형이 벡터들에 "과대적합(overfitting)"하는 결과를 피하기가 거의 불가능. 벡터의 차원이 말뭉치의 문서 수보다 크므로 어쩔수 없는 일.
- 다음 장에서는 문서 빈도 통계량들을 이용, 기계 학습에 유용하지 않은 희소 n-그램들을 골라내는 방법을 논의. 일반적으로, 너무 드문(이를 테면 세개 미만의 문서에만 출현하는)n-그램은 생략. 이는 앞의 동전 분류기에서 이야기한 '드문 단어 필터'에 해당.
- 이번에는 반대의 문제점을 생각. 예제 출력의 "at the"라는 2-그램은 드문 단어 조합이 아니지만, 너무 많은 문서에 등장해서 문제.
- 이런 단어 조합은 문서의 의미를 구분하는 데 도움이 되지 않음. 단어나 그밖의 토큰도 마찬가지며, 너무 자주 나오는 n-그램도 걸러내야 함. 예를 들면 말뭉치의 문서 중 25% 이상에 등장하는 토큰이나 n-그램은 무시하는 것이 좋음. 이는 앞의 동전분류기에서 이야기한 "불용어"필터에 해당. 불용어 필터는 개별 토큰뿐만 아니라 n-그램에도 적용할 수 있으며, 개별 토큰보다 n-그램에 더욱 유용.