# 1.NLP 기초.
## 2. 단어 토큰화
### 2. 토큰 생성기를 이용한 어휘 구축 - 이어서
- 이러한 원 핫 단어 벡터 표현은 원문의 ㅗ든 세부 사항(문법, 단어 순서등)을 유지. 그러면서도 컴퓨터가 '이해'할 수 있는 수치 자료의 형태. 게다가 이 수치 자료는 컴퓨터가 아주 자연스럽게 다루는 이진수로 이루어져 있음.
- 그러나 짧은 문장임에도 테이블이 꽤 큼. 테이블을 저장한 파일은 원래 문서를 담은 파일보다 훨씬 크고, 만일 원래 문서가 아주 길다면 테이블 파일은 감당할 수 없을 만큼 커짐.영어에서 일상적으로 쓰이는 단어는 적어도 2만개, 사람 이름같은 고유 명사 포함 시 수백만 개이고. 처리할 모든 문서마다 새로운 원핫 벡터 테이블(행렬)이 필요. 이는 마치 문서의 원본'이미지(영상)'을 뜨는 것과 거의 비슷. 이미지 자료에서 유용한 정보를 추출하듯 차원 축소 과정이 필요.
- 이런 파일이 얼마나 커질 수 있는지 계산해보면, 일반적으로 NLP 파이프라인에서 사용하는 어휘의 토큰 수는 2만개를 넘고, 수십만은 물론, 수백만개일 때도 있음. 일단은 어휘의 토큰 수가 1백만개라고 가정, 책 한 권이 3,500개의 문장으로 이루어지며 한 문장 단어 수가 15개이라고 할때.
- 이런 책을 3000권 처리하는데 필요한 원핫 벡터 테이블의 크기는 테이블의 행 수는 3000^2*15=157500000개, 바이트 수는 각 칸이 1바이트일때 이에 1백만을 곱하고, GB로 계산하면 157500GB, TB로 계산하면 157.5TB가 된다.
- 이 책들을 처리하려면 수조 개의 테이블 칸이 필요하고, 몇백권의 책을 처리하는데는 몇십테라바이트가 필요.
- 모든 문서를 디스크에 저장할 필요는 없고, 이런 자료 구조는 문서를 하나씩 처리할 때 일시적으로 RAM안에만 존재하면 됨.
- 어쨌거나, 이 모든 0과 문서의 단어 순서들을 일일이 저장하려 드는 것은 합리적이지 않고, 때에 따라서는 현실적으로 불가능.
- 우리가 원하는 것은 문서의 의미를 압축, 그 본질(essence;정수)을 추출하는 것. 즉, 우리는 문서 하나를 커다란 테이블이 아니라 벡터 하나로 압축, 완벽한 '복원'을 포기하고 주요 의미(정보)를 포착하려고 하는 것.
- 문서를 그보다 짧은 의미 단위, 즉 문장들로 분할하고. 한 문장의 단어들만으로도(즉 , 다른 어떤 문맥 정보 없이도) 그 문장의 주요 의미를 뽑아낼 수 있다고 가성. 단어들의 순서와 문법을 무시하고 단어들을 하나의 단어 모음 자료 구조에 넣고, 문장 또는 짧은 문서마다 그런 단어 모음 표현을 생성한다고 가정.
- 이런 단어 모음벡터는 문서에 담긴 정보 내용을 좀 더 다루기 쉬운 자료 구조로 압축하는 좋은 수단.
- 이러한 단어 모음 벡터를 단어 빈도 벡터라고도 함. 이는 이 벡터가 각 단어의 빈도(frequency;도수),즉 출현 횟수만 담고 있기 때문. 원핫 벡터와는 달리 이 단어 모음 벡터에는 단어들의 순서에 관한 정보가 없으며, 문서를 '재생'하지 못함. 대신 하나의 문서 또는 문장 전체를 단 하나의, 그리고 적당한 길이의 벡터로 표현할 수 있다는 장점. 단어 모음 벡터의 길이는 어휘의 크기(처리하고자 하는 고유한 토큰 개수)를 넘지 않음.